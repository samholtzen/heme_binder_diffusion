{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e578ba7a-6cc5-4cb7-9bd8-d63dbaba38af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "from shutil import copy2\n",
    "import random\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "### Path to this cloned GitHub repo:\n",
    "SCRIPT_DIR = os.path.dirname('/home/sholtzen/heme_binder_diffusion/')  # edit this to the GitHub repo path. Throws an error by default.\n",
    "assert os.path.exists(SCRIPT_DIR)\n",
    "sys.path.append(f\"/home/sholtzen/heme_binder_diffusion/scripts/utils/\")\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25115bb",
   "metadata": {},
   "source": [
    "The pipeline consists of 7 steps:<br>\n",
    "\n",
    "    0) The protein backbones are generated with RFdiffusionAA\n",
    "    1) Sequence is designed with proteinMPNN (without the ligand)\n",
    "    2) Structures are predicted with AlphaFold2\n",
    "    3) Ligand binding site is designed with LigandMPNN/FastRelax, or Rosetta FastDesign\n",
    "    4) Sequences surrounding the ligand pocket are diversified with LigandMPNN\n",
    "    5) Final designed sequences are predicted with AlphaFold2\n",
    "    6) Alphafold2-predicted models are relaxed with the ligand and analyzed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49d9e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_script = f\"{SCRIPT_DIR}/rf_diffusion_all_atom/run_inference.py\"  # edit this\n",
    "inpaint_script = f\"{SCRIPT_DIR}/RFDesign/inpainting/inpaint.py\"  # edit this if needed\n",
    "proteinMPNN_script = f\"{SCRIPT_DIR}/lib/LigandMPNN/run.py\"  # from submodule\n",
    "AF2_script = f\"{SCRIPT_DIR}/scripts/af2/af2.py\"  # from submodule\n",
    "\n",
    "### Python and/or Apptainer executables needed for running the jobs\n",
    "### Please provide paths to executables that are able to run the different tasks.\n",
    "### They can all be the same if you have an environment with all of the ncessary Python modules in one\n",
    "\n",
    "# If your added Apptainer does not execute scripts directly,\n",
    "# try adding 'apptainer run' or 'apptainer run --nv' (for GPU) in front of the command\n",
    "\n",
    "CONDAPATH = \"/home/sholtzen/miniforge3\"   # edit this depending on where your Conda environments live\n",
    "PYTHON = {\"diffusion\": f\"{CONDAPATH}/envs/diffusion/bin/python\",\n",
    "          \"af2\": f\"{CONDAPATH}/envs/mlfold/bin/python\",\n",
    "          \"proteinMPNN\": f\"{CONDAPATH}/envs/diffusion/bin/python\",\n",
    "          \"general\": f\"{CONDAPATH}/envs/diffusion/bin/python\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4990c-bff7-44dd-b675-c0379d9a45af",
   "metadata": {},
   "source": [
    "## Project description and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cfb5a8-5f64-49f4-a26f-0da5f09b4120",
   "metadata": {},
   "outputs": [],
   "source": [
    "username='sholtzen'\n",
    "LIGAND = \"ACO\"\n",
    "\n",
    "### Path where the jobs will be run and outputs dumped\n",
    "WDIR = f\"{SCRIPT_DIR}/{LIGAND}_binders/\"\n",
    "\n",
    "if not os.path.exists(WDIR):\n",
    "    os.makedirs(WDIR, exist_ok=True)\n",
    "\n",
    "print(f\"Working directory: {WDIR}\")\n",
    "\n",
    "USE_GPU_for_AF2 = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fca3ef-ba68-4896-8a98-dcc881832174",
   "metadata": {},
   "source": [
    "## 0: Setting up diffusion run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9faacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using example PDB file with ligand HBA and protein 7o2g backbone.\n",
    "## Note: the repository also contains additional HBA conformers with 7o2g and P450 motifs\n",
    "## in the same directory as a ZIP file.\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "params = [f\"{SCRIPT_DIR}/input/params/{LIGAND}/{LIGAND}.params\"]  # Rosetta params file(s)\n",
    "\n",
    "diffusion_inputs = glob.glob(f\"{SCRIPT_DIR}/input/pdbs/{LIGAND}/*_{LIGAND}.pdb\")\n",
    "\n",
    "diffusion_idx = random.sample(range(len(diffusion_inputs)), 20)\n",
    "diffusion_inputs = [diffusion_inputs[i] for i in diffusion_idx]\n",
    "\n",
    "print(f\"Found {len(diffusion_inputs)} PDB files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc22697-cfe5-412a-91bb-483826face14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Setting up general settings for diffusion\n",
    "DIFFUSION_DIR = f\"{WDIR}0_diffusion\"\n",
    "print(DIFFUSION_DIR)\n",
    "\n",
    "if not os.path.exists(DIFFUSION_DIR):\n",
    "    os.makedirs(DIFFUSION_DIR, exist_ok=False)\n",
    "\n",
    "os.chdir(DIFFUSION_DIR)\n",
    "\n",
    "N_designs = 30\n",
    "T_steps = 200\n",
    "\n",
    "## Edit this config based on motif residues, etc...\n",
    "config = f\"\"\"\n",
    "defaults:\n",
    "  - aa\n",
    "  - _self_\n",
    "\n",
    "diffuser:\n",
    "  T: {T_steps}\n",
    "\n",
    "inference:\n",
    "  num_designs: {N_designs}\n",
    "  model_runner: NRBStyleSelfCond\n",
    "  ligand: '{LIGAND}'\n",
    "  cuda_core: 0\n",
    "\n",
    "model:\n",
    "  freeze_track_motif: True\n",
    "\n",
    "contigmap:\n",
    "  contigs: [\"120-170\"]\n",
    "  inpaint_str: null\n",
    "\n",
    "potentials:\n",
    "  guiding_potentials: [\"type:ligand_ncontacts,weight:2\"] \n",
    "  guide_scale: 2\n",
    "  guide_decay: quadratic\n",
    "\"\"\"\n",
    "\n",
    "# assuming 8 seconds per timestep on GTX 1080 and batch size of 3 across two GPUs\n",
    "estimated_time = 13 * T_steps * N_designs * len(diffusion_inputs) / 10 / 2\n",
    "\n",
    "print(f\"Estimated time to produce {N_designs * len(diffusion_inputs)} designs = {estimated_time/3600:.2f} hours\")\n",
    "with open(\"config.yaml\", \"w\") as file:\n",
    "    file.write(config)\n",
    "print(f\"Wrote config file to {os.path.realpath('config.yaml')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5a77ad-3755-47b2-9451-955a9efea530",
   "metadata": {},
   "outputs": [],
   "source": [
    "commands_diffusion = []\n",
    "cmds_filename = \"commands_diffusion\"\n",
    "diffusion_rundirs = []\n",
    "\n",
    "\n",
    "with open(cmds_filename, \"w\") as file:\n",
    "    for p in diffusion_inputs:\n",
    "        pdbname = os.path.basename(p).replace(\".pdb\", \"\")\n",
    "        os.makedirs(pdbname, exist_ok=True)\n",
    "        cmd = f\"cd {pdbname} ; {PYTHON['diffusion']} {diffusion_script} --config-dir={DIFFUSION_DIR} \"\\\n",
    "              f\"--config-name=config.yaml inference.input_pdb={p} \"\\\n",
    "              f\"inference.output_prefix='./out/{pdbname}_dif' \"\n",
    "        commands_diffusion.append(cmd)\n",
    "        diffusion_rundirs.append(pdbname)\n",
    "        file.write(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a89617-a8a7-45d5-abd5-dc85fa32bc17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Setting up diffusion commands based on the input PDB file(s)\n",
    "## Diffusion jobs are run in separate directories for each input PDB\n",
    "from threadpoolctl import threadpool_limits, threadpool_info\n",
    "\n",
    "import time\n",
    "\n",
    "# Rudimentary CUDA scheduler...\n",
    "\n",
    "# Instantiate cuda and process lists\n",
    "\n",
    "batch_size = 10\n",
    "num_cuda_cores = [5,6]\n",
    "\n",
    "open_cuda = []\n",
    "taken_cuda = []\n",
    "processes = []\n",
    "\n",
    "# Create open cuda list with all avaialble cuda cores\n",
    "for cuda in num_cuda_cores:\n",
    "    for batch in range(batch_size):\n",
    "        open_cuda.append(cuda)  \n",
    "\n",
    "        \n",
    "# Iterate through `commands_diffusion` variable until there are no commands left\n",
    "while commands_diffusion:\n",
    "\n",
    "    # Take a census of the open CUDA to see that they're not at batch size\n",
    "    census_cuda = [taken_cuda.count(0), taken_cuda.count(1)]\n",
    "    for i, ele in enumerate(census_cuda):\n",
    "        print(f'{ele} processes on core {i}.') \n",
    "    \n",
    "    cuda_maxed = all(ele==batch_size for ele in census_cuda)\n",
    "\n",
    "    # iterate through the open cuda cores for as long as there are open cores and they're not maxed\n",
    "    while open_cuda and not cuda_maxed and commands_diffusion:\n",
    "\n",
    "        # Pluck command and cuda core pair\n",
    "        command = commands_diffusion.pop(0)\n",
    "        cuda_core = open_cuda.pop(0)\n",
    "\n",
    "        # Assign that cuda core to the list of taken cores\n",
    "        taken_cuda.append(cuda_core)        \n",
    "\n",
    "        # Write commands with that assigned core and create a subprocess on that core\n",
    "        command += f\"inference.cuda_core={cuda_core} \"\n",
    "        command += \"> output.log ; cd ..\\n\"\n",
    "        with threadpool_limits(limits=1):\n",
    "            processes.append(subprocess.Popen(command, shell=True))\n",
    "\n",
    "        print(f'Diffusing backbone based on PDB {command[3:7]}, running on CUDA core {cuda_core}...')\n",
    "\n",
    "    # Sleep while all processes are running\n",
    "    while all([process.poll() is None for process in processes]):\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Once any process is done, identify that process as the index of the process list (and by extension, the cuda core it's assigned to)\n",
    "    done_cuda = [i for i, x in enumerate(processes) if x.poll() is not None]\n",
    "\n",
    "    print(f'Process {done_cuda[0]} is done, switching...')\n",
    "\n",
    "    # Get out the index of that process's cuda core in the `taken_cuda` variable\n",
    "    index_cuda = done_cuda.pop(0)\n",
    "    processes.pop(index_cuda)\n",
    "\n",
    "    # Which cuda core are we talking about?\n",
    "    move_cuda = taken_cuda[index_cuda]\n",
    "\n",
    "    # Append `move_cuda` to `open_cuda` and remove that core from `taken_cuda`\n",
    "    open_cuda.append(move_cuda)\n",
    "    taken_cuda.pop(index_cuda)\n",
    "\n",
    "print('All done!')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed7eb2-c045-49a0-9264-292a6ef4dc27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## If you're done with diffusion and happy with the outputs then mark it as done\n",
    "DIFFUSION_DIR = f\"{WDIR}/0_diffusion\"\n",
    "os.chdir(DIFFUSION_DIR)\n",
    "\n",
    "if not os.path.exists(DIFFUSION_DIR+\"/.done\"):\n",
    "    with open(f\"{DIFFUSION_DIR}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a474a099-60e1-493d-85f2-12b8808a0b11",
   "metadata": {},
   "source": [
    "### Analyzing diffusion outputs\n",
    "The purpose of this step is to identify diffused backbones that meet certain quality criteria. These scaffolds should be relatively globular (measured by radius of gyration (rog), and longest helix). They should not have clashes between the ligand and the backbone, the ligand should not be too exposed (measured by relative SASA). The termini should not be too close to the ligand (term_mindist), and the backbone should not be too loopy. In the example below we are also looking for backbones that leave some part of the ligand more exposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c85384-baa4-41c2-bc21-42587839a44d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Analyzing diffusion outputs for clashes, ligand burial and scaffold quality\n",
    "## If it's running too slowly consider increasing --nproc\n",
    "analysis_script = f\"{SCRIPT_DIR}/scripts/diffusion_analysis/process_diffusion_outputs.py\"\n",
    "\n",
    "diffusion_outputs = []\n",
    "diffusion_count = 0\n",
    "for d in diffusion_rundirs:\n",
    "    diffusion_outputs += glob.glob(f\"{d}/out/*.pdb\")\n",
    "    diffusion_count += len(glob.glob(f\"{d}/out/*.pdb\"))\n",
    "    \n",
    "print(f\"{diffusion_count} backbones to analyze\")\n",
    "\n",
    "# By default I don't use the --analyze flag. As a result the backbones are filtered as the script runs.\n",
    "\n",
    "dif_analysis_cmd_dict = {\"--pdb\": \" \".join(diffusion_outputs),\n",
    "                        \"--params\": \" \".join(params),\n",
    "                        \"--term_limit\": \"10.0\",\n",
    "                        \"--SASA_limit\": \"0.4\",  # Highest allowed relative SASA of ligand\n",
    "                        \"--loop_limit\": \"0.4\",  # Fraction of backbone that can be loopy\n",
    "                        \"--rethread\": True,\n",
    "                        \"--longest_helix\": \"30\",\n",
    "                        \"--rog\": \"30.0\",\n",
    "                        \"--fix\": True,\n",
    "                        \"--partial\": None,\n",
    "                        \"--outdir\": None,\n",
    "                        \"--traj\": \"5/30\",  # Also random 5 models are taken from the last 30 steps of the diffusion trajectory\n",
    "                        \"--analyze\": False,\n",
    "                        \"--nproc\": \"8\"}\n",
    "\n",
    "analysis_command = f\"{PYTHON['general']} {analysis_script}\"\n",
    "\n",
    "for k, val in dif_analysis_cmd_dict.items():\n",
    "    if val is not None:\n",
    "        if isinstance(val, list):\n",
    "            analysis_command += f\" {k}\"\n",
    "            analysis_command += \" \" + \" \".join(val)\n",
    "        elif isinstance(val, bool):\n",
    "            if val == True:\n",
    "                analysis_command += f\" {k}\"\n",
    "        else:\n",
    "            analysis_command += f\" {k} {val}\"\n",
    "\n",
    "p = subprocess.Popen(analysis_command, shell=True)\n",
    "(output, err) = p.communicate()\n",
    "\n",
    "diffused_backbones_good = glob.glob(f\"{DIFFUSION_DIR}/filtered_structures/*.pdb\")\n",
    "\n",
    "dif_analysis_df = pd.read_csv(f\"{DIFFUSION_DIR}/diffusion_analysis.sc\", header=0, sep=r\"\\s+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10b4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualizing the distributions of diffusion analysis metrics\n",
    "## Plotting design scores\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i,k in enumerate(dif_analysis_df.keys()):\n",
    "    if k in [\"description\"]:\n",
    "        continue\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    plt.hist(dif_analysis_df[k])\n",
    "    plt.title(k)\n",
    "    plt.xlabel(k)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6136b4",
   "metadata": {},
   "source": [
    "It is highly advised that you manually inspect the filtered diffusion outputs before continuing with the pipeline.\n",
    "While the filters attempt to pick out the most offending designs then nothing beats your own intuition and judgement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9561c720",
   "metadata": {},
   "source": [
    "If you would like to perform RFjoint Inpainting on the diffusion outputs, please go to the [inpainting section](#inpainting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7182f5c5-ab5c-4f98-b007-3f72da606147",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93a2f55",
   "metadata": {},
   "source": [
    "## 1: Running ProteinMPNN on diffused backbones\n",
    "\n",
    "We are first trying to just design a sequence on the backbone, without considering the ligand.\n",
    "The goal is to first find backbones that fold well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff795c-8b28-4232-8d0f-e308d1041404",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffused_backbones_good = glob.glob(f\"{DIFFUSION_DIR}/filtered_structures/*.pdb\")\n",
    "assert len(diffused_backbones_good) > 0, \"No good backbones found!\"\n",
    "\n",
    "os.chdir(WDIR)\n",
    "\n",
    "MPNN_DIR = f\"{WDIR}/1_proteinmpnn\"\n",
    "os.makedirs(MPNN_DIR, exist_ok=True)\n",
    "os.chdir(MPNN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235734c-e319-48cd-80e2-f6c5e0cacfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parsing diffusion output TRB files to extract fixed motif residues\n",
    "## These residues will not be redesigned with proteinMPNN\n",
    "mask_json_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/design/make_maskdict_from_trb.py --out masked_pos.jsonl --trb\"\n",
    "for d in diffused_backbones_good:\n",
    "    mask_json_cmd += \" \" + d.replace(\".pdb\", \".trb\")\n",
    "p = subprocess.Popen(mask_json_cmd, shell=True)\n",
    "(output, err) = p.communicate()\n",
    "\n",
    "assert os.path.exists(\"masked_pos.jsonl\"), \"Failed to create masked positions JSONL file\"\n",
    "\n",
    "\n",
    "### Setting up proteinMPNN run commands\n",
    "## We're doing design with 3 temperatures, and 5 sequences each.\n",
    "## This usually gives decent success with designable backbones.\n",
    "## For more complicated cases consider doing >100 sequences.\n",
    "\n",
    "MPNN_temperatures = [0.1, 0.2, 0.3]\n",
    "MPNN_outputs_per_temperature = 5\n",
    "MPNN_omit_AAs = \"M\"\n",
    "\n",
    "commands_mpnn = []\n",
    "for T in MPNN_temperatures:\n",
    "    for f in diffused_backbones_good:\n",
    "        commands_mpnn.append(f\"{PYTHON['proteinMPNN']} {proteinMPNN_script} \"\n",
    "                             f\"--model_type protein_mpnn --ligand_mpnn_use_atom_context 0 \"\n",
    "                             \"--fixed_residues_multi masked_pos.jsonl --out_folder ./ \"\n",
    "                             f\"--number_of_batches {MPNN_outputs_per_temperature} --temperature {T} \"\n",
    "                             f\"--omit_AA {MPNN_omit_AAs} --pdb_path {f} \"\n",
    "                             f\"--checkpoint_protein_mpnn {SCRIPT_DIR}/lib/LigandMPNN/model_params/proteinmpnn_v_48_020.pt \")\n",
    "\n",
    "print(\"Example MPNN command:\")\n",
    "print(commands_mpnn[-1])\n",
    "\n",
    "### Running proteinMPNN with Slurm.\n",
    "### Grouping jobs with 10 commands per one array job.\n",
    "batch_size = 10\n",
    "num_cuda_cores = [5,6]\n",
    "\n",
    "open_cuda = []\n",
    "taken_cuda = []\n",
    "processes = []\n",
    "\n",
    "# Create open cuda list with all avaialble cuda cores\n",
    "for cuda in num_cuda_cores:\n",
    "    for batch in range(batch_size):\n",
    "        open_cuda.append(cuda)  \n",
    "\n",
    "        \n",
    "# Iterate through `commands_diffusion` variable until there are no commands left\n",
    "while commands_mpnn:\n",
    "\n",
    "    # Take a census of the open CUDA to see that they're not at batch size\n",
    "    census_cuda = [taken_cuda.count(0), taken_cuda.count(1)]\n",
    "    for i, ele in enumerate(census_cuda):\n",
    "        print(f'{ele} processes on core {i}.') \n",
    "    \n",
    "    cuda_maxed = all(ele==batch_size for ele in census_cuda)\n",
    "\n",
    "    # iterate through the open cuda cores for as long as there are open cores and they're not maxed\n",
    "    while open_cuda and not cuda_maxed and commands_mpnn:\n",
    "\n",
    "        # Pluck command and cuda core pair\n",
    "        command = commands_mpnn.pop(0)\n",
    "        cuda_core = open_cuda.pop(0)\n",
    "\n",
    "        # Assign that cuda core to the list of taken cores\n",
    "        taken_cuda.append(cuda_core)        \n",
    "\n",
    "        # Write commands with that assigned core and create a subprocess on that core\n",
    "        command += f\"--cuda_core {cuda_core} \"\n",
    "        with threadpool_limits(limits=1):\n",
    "            processes.append(subprocess.Popen(command, shell=True))\n",
    "\n",
    "        print(f'Diffusing backbone based on PDB {command[3:7]}, running on CUDA core {cuda_core}...')\n",
    "\n",
    "    # Sleep while all processes are running\n",
    "    while all([process.poll() is None for process in processes]):\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Once any process is done, identify that process as the index of the process list (and by extension, the cuda core it's assigned to)\n",
    "    done_cuda = [i for i, x in enumerate(processes) if x.poll() is not None]\n",
    "\n",
    "    print(f'Process {done_cuda[0]} is done, switching...')\n",
    "\n",
    "    # Get out the index of that process's cuda core in the `taken_cuda` variable\n",
    "    index_cuda = done_cuda.pop(0)\n",
    "    processes.pop(index_cuda)\n",
    "\n",
    "    # Which cuda core are we talking about?\n",
    "    move_cuda = taken_cuda[index_cuda]\n",
    "\n",
    "    # Append `move_cuda` to `open_cuda` and remove that core from `taken_cuda`\n",
    "    open_cuda.append(move_cuda)\n",
    "    taken_cuda.pop(index_cuda)\n",
    "\n",
    "print('All done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f793273",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you're done with diffusion and happy with the outputs then mark it as done\n",
    "MPNN_DIR = f\"{WDIR}/1_proteinmpnn\"\n",
    "os.chdir(MPNN_DIR)\n",
    "\n",
    "if not os.path.exists(MPNN_DIR+\"/.done\"):\n",
    "    with open(f\"{MPNN_DIR}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97333709",
   "metadata": {},
   "source": [
    "## 2: Running AlphaFold2\n",
    "Performing AF2 single sequence predictions.<br>\n",
    "By default only using AF2 model 4 with 3 recycles. For more complicated folds 10+ recycles might be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065029b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WDIR)\n",
    "\n",
    "AF2_DIR = f\"{WDIR}/2_af2\"\n",
    "os.makedirs(AF2_DIR, exist_ok=True)\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "### First collecting MPNN outputs and creating FASTA files for AF2 input\n",
    "mpnn_fasta = utils.parse_fasta_files(glob.glob(f\"{MPNN_DIR}/seqs/*.fa\"))\n",
    "mpnn_fasta = {k: seq.strip() for k, seq in mpnn_fasta.items() if \"model_path\" not in k}  # excluding the diffused poly-A sequence\n",
    "# Giving sequences unique names based on input PDB name, temperature, and sequence identifier\n",
    "mpnn_fasta = {k.split(\",\")[0]+\"_\"+k.split(\",\")[2].replace(\" T=\", \"T\")+\"_0_\"+k.split(\",\")[1].replace(\" id=\", \"\"): seq for k, seq in mpnn_fasta.items()}\n",
    "\n",
    "print(f\"A total on {len(mpnn_fasta)} sequences will be predicted.\")\n",
    "\n",
    "## Splitting the MPNN sequences based on length\n",
    "## and grouping them in smaller batches for each AF2 job\n",
    "## Use group size of >40 when running on GPU. Also depends on how many sequences and resources you have.\n",
    "\n",
    "with open('all_seq.fasta', 'w') as file:\n",
    "    for k in mpnn_fasta:\n",
    "        file.write(f\"{k}\\n{mpnn_fasta[k]}\\n\")\n",
    "\n",
    "SEQUENCES_PER_AF2_JOB = 100  # GPU\n",
    "mpnn_fasta_split = utils.split_fasta_based_on_length(mpnn_fasta, SEQUENCES_PER_AF2_JOB, write_files=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf52a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up AlphaFold2 run\n",
    "from threadpoolctl import threadpool_limits\n",
    "AF2_recycles = 3\n",
    "AF2_models = \"4\"  # add other models to this string if needed, i.e. \"3 4 5\"\n",
    "\n",
    "commands_af2 = []\n",
    "cmds_filename_af2 = \"commands_af2\"\n",
    "with open(cmds_filename_af2, \"w\") as file:\n",
    "    for ff in glob.glob(\"*.fasta\"):\n",
    "        commands_af2.append(f\"{PYTHON['af2']} {AF2_script} \"\n",
    "                             f\"--af-nrecycles {AF2_recycles} --af-models {AF2_models} \"\n",
    "                             f\"--fasta {ff} --scorefile {ff.replace('.fasta', '.csv')}\\n\")\n",
    "        file.write(commands_af2[-1])\n",
    "\n",
    "print(\"Example AF2 command:\")\n",
    "print(commands_af2[-1]) \n",
    "\n",
    "with open(cmds_filename_af2, 'r') as file:\n",
    "    for line in file:\n",
    "        with threadpool_limits(limits=1):\n",
    "            p = subprocess.Popen(line, shell=True)\n",
    "            (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a88b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you're done with diffusion and happy with the outputs then mark it as done\n",
    "AF2_DIR = f\"{WDIR}/2_af2\"\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "if not os.path.exists(AF2_DIR+\"/.done\"):\n",
    "    with open(f\"{AF2_DIR}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e9def",
   "metadata": {},
   "source": [
    "### Analyzing AlphaFold2 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f38048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all CSV scorefiles into one\n",
    "AF2_DIR = f\"{WDIR}/2_af2\"\n",
    "DIFFUSION_DIR = f\"{WDIR}/0_diffusion\"\n",
    "\n",
    "os.system(\"head -n 1 $(ls all_seq.csv | shuf -n 1) > scores.csv ; for f in all_seq.csv ; do tail -n +2 ${f} >> scores.csv ; done\")\n",
    "assert os.path.exists(\"scores.csv\"), \"Could not combine scorefiles\"\n",
    "\n",
    "### Calculating the RMSDs of AF2 predictions relative to the diffusion outputs\n",
    "### Catalytic residue sidechain RMSDs are calculated in the reference PDB has REMARK 666 line present\n",
    "\n",
    "analysis_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/utils/analyze_af2.py --scorefile scores.csv \"\\\n",
    "               f\"--ref_path {DIFFUSION_DIR}/filtered_structures/ --mpnn --params {' '.join(params)} --nproc 4\"\n",
    "\n",
    "## Analyzing locally\n",
    "p = subprocess.Popen(analysis_cmd, shell=True)\n",
    "(output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c18f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_af2 = pd.read_csv(\"scores.sc\", sep=r\"\\s+\", header=0)\n",
    "\n",
    "### Filtering AF2 scores based on lddt and rmsd\n",
    "# Define your desired cutoffs here:\n",
    "AF2_filters = {\"lDDT\": [80.0, \">=\"],\n",
    "               \"rmsd\": [3, \"<=\"]}  # 1st catalytic residue sc-rmsd\n",
    "\n",
    "scores_af2_filtered = utils.filter_scores(scores_af2, AF2_filters)\n",
    "utils.dump_scorefile(scores_af2_filtered, \"filtered_scores.sc\")\n",
    "\n",
    "## Plotting AF2 scores\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i,k in enumerate(AF2_filters):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.hist(scores_af2[k])\n",
    "    plt.title(k)\n",
    "    plt.xlabel(k)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "utils.plot_score_pairs(scores_af2, \"lDDT\", \"rmsd\", AF2_filters[\"lDDT\"][0], AF2_filters[\"rmsd\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeb0952",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Copying good predictions to a separate directory\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "if len(scores_af2_filtered) > 0:\n",
    "    os.makedirs(\"good\", exist_ok=True)\n",
    "    good_af2_models = [row[\"Output_PDB\"]+\".pdb\" for idx,row in scores_af2_filtered.iterrows()]\n",
    "    for pdb in good_af2_models:\n",
    "        copy2(pdb, f\"good/{pdb}\")\n",
    "    good_af2_models = glob.glob(f\"{AF2_DIR}/good/*.pdb\")\n",
    "else:\n",
    "    sys.exit(\"No good models to continue this pipeline with\")\n",
    "\n",
    "os.chdir(f\"{AF2_DIR}/good\")\n",
    "\n",
    "\n",
    "### Aligning the ligand back into the AF2 predictions.\n",
    "### This is done by aligning the AF2 model to diffusion output and copying over the ligand using PyRosetta.\n",
    "### --fix_catres option will readjust the rotamer and tautomer of \n",
    "### any catalytic residue to be the same as in the reference model.\n",
    "\n",
    "align_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/utils/place_ligand_after_af2.py \"\\\n",
    "            f\"--outdir with_{LIGAND} --params {' '.join(params)} --fix_catres \"\\\n",
    "            f\"--pdb {' '.join(good_af2_models)} \"\\\n",
    "            f\"--ref {' '.join(glob.glob(DIFFUSION_DIR+'/filtered_structures/*.pdb'))}\"\n",
    "\n",
    "p = subprocess.Popen(align_cmd, shell=True)\n",
    "(output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac54ca5",
   "metadata": {},
   "source": [
    "## 3.1: Performing binding site design with ligandMPNN / FastRelax\n",
    "(see 3.2 down below for design with [Rosetta FastDesign](#Rosetta_design))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e730b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting up design directory and commands\n",
    "os.chdir(WDIR)\n",
    "DESIGN_DIR_ligMPNN = f\"{WDIR}/3.1_design_pocket_ligandMPNN\"\n",
    "os.makedirs(DESIGN_DIR_ligMPNN, exist_ok=True)\n",
    "os.chdir(DESIGN_DIR_ligMPNN)\n",
    "\n",
    "AF2_DIR = f\"{WDIR}/2_af2\"\n",
    "os.makedirs(DESIGN_DIR_ligMPNN+\"/logs\", exist_ok=True)\n",
    "\n",
    "### Performing 5 design iterations on each input structure\n",
    "\n",
    "NSTRUCT = 5\n",
    "commands_design = []\n",
    "cmds_filename_des = \"commands_design\"\n",
    "with open(cmds_filename_des, \"w\") as file:\n",
    "    for pdb in glob.glob(f\"{AF2_DIR}/good/with_{LIGAND}/*.pdb\"):\n",
    "        commands_design.append(f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/design/heme_pocket_ligMPNN.py \"\n",
    "                             f\"--pdb {pdb} --nstruct {NSTRUCT} --design_full --nproc 2 \"\n",
    "                             f\"--scoring {SCRIPT_DIR}/scripts/design/scoring/heme_scoring.py \"\n",
    "                             f\"--params {' '.join(params)} > logs/{os.path.basename(pdb).replace('.pdb', '.log')}\\n\")\n",
    "        file.write(commands_design[-1])\n",
    "\n",
    "print(\"Example design command:\")\n",
    "print(commands_design[-1])\n",
    "\n",
    "import time\n",
    "\n",
    "processes=[]\n",
    "batch_size=10\n",
    "\n",
    "while commands_design:\n",
    "\n",
    "    # Take a census of the open processes to see that they're not at batch size\n",
    "\n",
    "    curr_proc = len(processes)\n",
    "    proc_maxed = curr_proc <= batch_size\n",
    "    \n",
    "    # iterate through the open cuda cores for as long as there are open cores and they're not maxed\n",
    "    while proc_maxed and commands_design:\n",
    "\n",
    "        # Pluck command and cuda core pair\n",
    "        command = commands_design.pop(0) \n",
    "        curr_proc = len(processes)\n",
    "        proc_maxed = curr_proc <= batch_size\n",
    "        print(f\"Running command:\\n{command}\")\n",
    "        processes.append(subprocess.Popen(command, shell=True))\n",
    "                \n",
    "    # Sleep while all processes are running\n",
    "    while all([process.poll() is None for process in processes]):\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Once any process is done, identify that process as the index of the process list\n",
    "    processes = [x for x in processes if x.poll() is None]\n",
    "    \n",
    "\n",
    "print('All done!')\n",
    "\n",
    "### Running design jobs with Slurm.\n",
    "# submit_script = \"submit_design.sh\"\n",
    "# utils.create_slurm_submit_script(filename=submit_script, name=\"3.1_design_pocket_ligMPNN\", mem=\"4g\", \n",
    "#                                  N_cores=1, time=\"3:00:00\", email=EMAIL, array=len(commands_design),\n",
    "#                                  array_commandfile=cmds_filename_des)\n",
    "\n",
    "# if not os.path.exists(DESIGN_DIR_ligMPNN+\"/.done\"):\n",
    "#     p = subprocess.Popen(['sbatch', submit_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "#     (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you're done with design and happy with the outputs then mark it as done\n",
    "DESIGN_DIR_ligMPNN = f\"{WDIR}/3.1_design_pocket_ligandMPNN\"\n",
    "os.chdir(DESIGN_DIR_ligMPNN)\n",
    "\n",
    "if not os.path.exists(DESIGN_DIR_ligMPNN+\"/.done\"):\n",
    "    with open(f\"{DESIGN_DIR_ligMPNN}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d7f43",
   "metadata": {},
   "source": [
    "### Analyzing ligMPNN / FastRelax design results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2364e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyzing ligMPNN / FastRelax designs\n",
    "if not os.path.exists(\"scorefile.txt\"):\n",
    "    sys.exit(\"Design job failed, or no successful outputs were produced.\")\n",
    "\n",
    "scores = pd.read_csv(\"scorefile.txt\", sep=\"\\s+\", header=0)\n",
    "\n",
    "filters = {\n",
    " 'nlr_totrms': [1.0, '<='],\n",
    " 'L_SASA': [0.2, '<='],\n",
    " 'score_per_res': [0.0, '<='],\n",
    " 'corrected_ddg': [-2.0, '<='],\n",
    " 'cms_per_atom': [4, '>=']}\n",
    "\n",
    "filtered_scores = utils.filter_scores(scores, filters)\n",
    "\n",
    "## Plotting AF2 scores\n",
    "plt.figure(figsize=(12, 9))\n",
    "for i,k in enumerate(filters):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.hist(scores[k])\n",
    "    plt.title(k)\n",
    "    plt.xlabel(k)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### Copying good designs over to a new directory\n",
    "if len(filtered_scores) > 0:\n",
    "    os.makedirs(f\"{DESIGN_DIR_ligMPNN}/good\")\n",
    "    for idx, row in filtered_scores.iterrows():\n",
    "        copy2(row[\"description\"]+\".pdb\", \"good/\"+row[\"description\"]+\".pdb\")\n",
    "else:\n",
    "    print(\"No good designs created, bummer...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c82f60",
   "metadata": {},
   "source": [
    "## 4.1 Performing ligandMPNN redesign on the 2nd layer residues\n",
    "\n",
    "Resampling residues that are not in the pocket, but also not very far from the pocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4626c9-4e5d-491a-863d-d450aed9dd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(glob.glob(DESIGN_DIR_ligMPNN+'/good/*.pdb')) == 0:\n",
    "    sys.exit(\"No designs to run 2nd MPNN on.\")\n",
    "\n",
    "os.chdir(WDIR)\n",
    "DESIGN_DIR_2nd_mpnn = f\"{WDIR}/4.1_2nd_mpnn\"\n",
    "os.makedirs(DESIGN_DIR_2nd_mpnn, exist_ok=True)\n",
    "os.chdir(DESIGN_DIR_2nd_mpnn)\n",
    "\n",
    "### Making a JSON file specifiying designable positions for each structure.\n",
    "### Will also make non-pocket ALA positions as designable.\n",
    "### This is to fix any surface ALA-patches that previous MPNN may have introduced.\n",
    "\n",
    "make_json_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/design/setup_ligand_mpnn_2nd_layer.py \"\\\n",
    "                f\"--params {' '.join(params)} --ligand {LIGAND} --output_path parsed_pdbs_lig.jsonl \"\\\n",
    "                 \"--output_path masked_pos.jsonl --dist_bb 6.0 --dist_sc 5.0 \"\\\n",
    "                f\"--pdb {' '.join(glob.glob(DESIGN_DIR_ligMPNN+'/good/*.pdb'))}\"\n",
    "\n",
    "p = subprocess.Popen(make_json_cmd, shell=True)\n",
    "(output, err) = p.communicate()\n",
    "\n",
    "### Setting up ligandMPNN run commands\n",
    "## We're doing design with 2 temperatures (more conservative than before), and 5 sequences each.\n",
    "\n",
    "MPNN_temperatures = [0.1, 0.2]\n",
    "MPNN_outputs_per_temperature = 5\n",
    "MPNN_omit_AAs = \"M\"\n",
    "\n",
    "commands_mpnn = []\n",
    "cmds_filename_mpnn = \"commands_mpnn\"\n",
    "with open(cmds_filename_mpnn, \"w\") as file:\n",
    "    for T in MPNN_temperatures:\n",
    "        for f in glob.glob(DESIGN_DIR_ligMPNN+'/good/*.pdb'):\n",
    "            commands_mpnn.append(f\"{PYTHON['proteinMPNN']} {proteinMPNN_script} \"\n",
    "                                 f\"--model_type ligand_mpnn --ligand_mpnn_use_atom_context 1 \"\n",
    "                                 \"--fixed_residues_multi masked_pos.jsonl --out_folder ./ \"\n",
    "                                 f\"--number_of_batches {MPNN_outputs_per_temperature} --temperature {T} \"\n",
    "                                 f\"--omit_AA {MPNN_omit_AAs} --pdb_path {f} \"\n",
    "                                 f\"--checkpoint_ligand_mpnn {SCRIPT_DIR}/lib/LigandMPNN/model_params/ligandmpnn_v_32_010_25.pt\\n\")\n",
    "            file.write(commands_mpnn[-1])\n",
    "\n",
    "print(\"Example MPNN command:\")\n",
    "print(commands_mpnn[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f470af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Running ligandMPNN with Slurm.\n",
    "### Grouping jobs with 10 commands per one array job.\n",
    "batch_size = 10\n",
    "num_cuda_cores = [6,7]\n",
    "\n",
    "open_cuda = []\n",
    "taken_cuda = []\n",
    "processes = []\n",
    "\n",
    "# Create open cuda list with all avaialble cuda cores\n",
    "for cuda in num_cuda_cores:\n",
    "    for batch in range(batch_size):\n",
    "        open_cuda.append(cuda)  \n",
    "\n",
    "        \n",
    "# Iterate through `commands_diffusion` variable until there are no commands left\n",
    "while commands_mpnn:\n",
    "\n",
    "    # Take a census of the open CUDA to see that they're not at batch size\n",
    "    census_cuda = [taken_cuda.count(0), taken_cuda.count(1)]\n",
    "    for i, ele in enumerate(census_cuda):\n",
    "        print(f'{ele} processes on core {i}.') \n",
    "    \n",
    "    cuda_maxed = all(ele==batch_size for ele in census_cuda)\n",
    "\n",
    "    # iterate through the open cuda cores for as long as there are open cores and they're not maxed\n",
    "    while open_cuda and not cuda_maxed and commands_mpnn:\n",
    "\n",
    "        # Pluck command and cuda core pair\n",
    "        command = commands_mpnn.pop(0)\n",
    "        cuda_core = open_cuda.pop(0)\n",
    "\n",
    "        # Assign that cuda core to the list of taken cores\n",
    "        taken_cuda.append(cuda_core)        \n",
    "\n",
    "        # Write commands with that assigned core and create a subprocess on that core\n",
    "        command += f\"--cuda_core {cuda_core} \"\n",
    "        with threadpool_limits(limits=1):\n",
    "            processes.append(subprocess.Popen(command, shell=True))\n",
    "\n",
    "        print(f'Diffusing backbone based on PDB {command[3:7]}, running on CUDA core {cuda_core}...')\n",
    "\n",
    "    # Sleep while all processes are running\n",
    "    while all([process.poll() is None for process in processes]):\n",
    "        time.sleep(1)\n",
    "\n",
    "    # Once any process is done, identify that process as the index of the process list (and by extension, the cuda core it's assigned to)\n",
    "    done_cuda = [i for i, x in enumerate(processes) if x.poll() is not None]\n",
    "\n",
    "    print(f'Process {done_cuda[0]} is done, switching...')\n",
    "\n",
    "    # Get out the index of that process's cuda core in the `taken_cuda` variable\n",
    "    index_cuda = done_cuda.pop(0)\n",
    "    processes.pop(index_cuda)\n",
    "\n",
    "    # Which cuda core are we talking about?\n",
    "    move_cuda = taken_cuda[index_cuda]\n",
    "\n",
    "    # Append `move_cuda` to `open_cuda` and remove that core from `taken_cuda`\n",
    "    open_cuda.append(move_cuda)\n",
    "    taken_cuda.pop(index_cuda)\n",
    "\n",
    "print('All done!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a60859c4-acfd-4fa7-a120-c28bde11a028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total on 984 sequences will be predicted.\n"
     ]
    }
   ],
   "source": [
    "## If you're done with design and happy with the outputs then mark it as done\n",
    "DESIGN_DIR_2nd_mpnn = f\"{WDIR}/4.1_2nd_mpnn\"\n",
    "os.chdir(DESIGN_DIR_2nd_mpnn)\n",
    "\n",
    "if not os.path.exists(DESIGN_DIR_2nd_mpnn+\"/.done\"):\n",
    "    with open(f\"{DESIGN_DIR_2nd_mpnn}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")\n",
    "\n",
    "os.chdir(WDIR)\n",
    "DESIGN_DIR_2nd_mpnn = f\"{WDIR}/4.1_2nd_mpnn\"\n",
    "assert os.path.exists(DESIGN_DIR_2nd_mpnn+\"/.done\"), \"2nd MPNN has not been performed!\"\n",
    "\n",
    "AF2_DIR = f\"{WDIR}/5.1_2nd_af2\"\n",
    "os.makedirs(AF2_DIR, exist_ok=True)\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "### First collecting MPNN outputs and creating FASTA files for AF2 input\n",
    "mpnn_fasta = utils.parse_fasta_files(glob.glob(f\"{DESIGN_DIR_2nd_mpnn}/seqs/*.fa\"))\n",
    "# Giving sequences unique names based on input PDB name, temperature, and sequence identifier\n",
    "_mpnn_fasta = {}\n",
    "for k, seq in mpnn_fasta.items():\n",
    "    if \"model_path\" in k:\n",
    "        _mpnn_fasta[k.split(\",\")[0]+\"_native\"] = seq.strip()\n",
    "    else:\n",
    "        _mpnn_fasta[k.split(\",\")[0]+\"_\"+k.split(\",\")[2].replace(\" T=\", \"T\")+\"_0_\"+k.split(\",\")[1].replace(\" id=\", \"\")] = seq.strip()\n",
    "mpnn_fasta = {k:v for k,v in _mpnn_fasta.items()}\n",
    "\n",
    "print(f\"A total on {len(mpnn_fasta)} sequences will be predicted.\")\n",
    "\n",
    "## Splitting the MPNN sequences based on length\n",
    "## and grouping them in smaller batches for each AF2 job\n",
    "## Use group size of >40 when running on GPU. Also depends on how many sequences and resources you have.\n",
    "\n",
    "with open('all_seq2.fasta', 'w') as file:\n",
    "    for k in mpnn_fasta:\n",
    "        file.write(f\"{k}\\n{mpnn_fasta[k]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd97aea",
   "metadata": {},
   "source": [
    "## 5.1 AlphaFold2 predictions on the 2nd MPNN run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f47c3e0-1305-486e-b97f-b6597150daa1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequence 136 completed in 5.1 sec with 1 models; lDDT=37.913\n",
      "Sequence 137 completed in 5.2 sec with 1 models; lDDT=38.176\n",
      "Sequence 138 completed in 5.3 sec with 1 models; lDDT=43.090\n",
      "Sequence 139 completed in 5.2 sec with 1 models; lDDT=55.186\n",
      "Sequence 140 completed in 5.2 sec with 1 models; lDDT=50.671\n",
      "Sequence 141 completed in 5.3 sec with 1 models; lDDT=52.217\n",
      "Sequence 142 completed in 5.2 sec with 1 models; lDDT=48.830\n",
      "Sequence 143 completed in 5.3 sec with 1 models; lDDT=52.607\n",
      "Sequence 144 completed in 62.6 sec with 1 models; lDDT=91.017\n",
      "Sequence 145 completed in 6.2 sec with 1 models; lDDT=89.424\n",
      "Sequence 146 completed in 5.9 sec with 1 models; lDDT=84.649\n",
      "Sequence 147 completed in 6.0 sec with 1 models; lDDT=78.948\n",
      "Sequence 148 completed in 5.8 sec with 1 models; lDDT=90.337\n",
      "Sequence 149 completed in 6.0 sec with 1 models; lDDT=90.040\n",
      "Sequence 150 completed in 5.8 sec with 1 models; lDDT=91.142\n",
      "Sequence 151 completed in 5.8 sec with 1 models; lDDT=95.013\n",
      "Sequence 152 completed in 5.5 sec with 1 models; lDDT=94.543\n",
      "Sequence 153 completed in 6.1 sec with 1 models; lDDT=94.011\n",
      "Sequence 154 completed in 5.9 sec with 1 models; lDDT=92.787\n",
      "Sequence 155 completed in 5.9 sec with 1 models; lDDT=94.424\n",
      "Sequence 156 completed in 6.0 sec with 1 models; lDDT=91.183\n",
      "Sequence 157 completed in 5.8 sec with 1 models; lDDT=91.532\n",
      "Sequence 158 completed in 6.0 sec with 1 models; lDDT=92.372\n",
      "Sequence 159 completed in 6.2 sec with 1 models; lDDT=92.990\n",
      "Sequence 160 completed in 5.6 sec with 1 models; lDDT=92.446\n",
      "Sequence 161 completed in 6.1 sec with 1 models; lDDT=90.675\n",
      "Sequence 162 completed in 5.9 sec with 1 models; lDDT=74.143\n",
      "Sequence 163 completed in 6.0 sec with 1 models; lDDT=89.334\n",
      "Sequence 164 completed in 6.1 sec with 1 models; lDDT=85.772\n",
      "Sequence 165 completed in 6.3 sec with 1 models; lDDT=88.889\n",
      "Sequence 166 completed in 5.8 sec with 1 models; lDDT=88.571\n",
      "Sequence 167 completed in 7.0 sec with 1 models; lDDT=91.202\n",
      "Sequence 168 completed in 5.8 sec with 1 models; lDDT=91.224\n",
      "Sequence 169 completed in 6.1 sec with 1 models; lDDT=90.852\n",
      "Sequence 170 completed in 5.6 sec with 1 models; lDDT=90.573\n",
      "Sequence 171 completed in 6.0 sec with 1 models; lDDT=91.578\n",
      "Sequence 172 completed in 5.7 sec with 1 models; lDDT=92.147\n",
      "Sequence 173 completed in 5.5 sec with 1 models; lDDT=91.766\n",
      "Sequence 174 completed in 59.3 sec with 1 models; lDDT=47.840\n",
      "Sequence 175 completed in 5.6 sec with 1 models; lDDT=71.293\n",
      "Sequence 176 completed in 5.5 sec with 1 models; lDDT=42.713\n",
      "Sequence 177 completed in 5.6 sec with 1 models; lDDT=52.596\n",
      "Sequence 178 completed in 5.9 sec with 1 models; lDDT=74.925\n",
      "Sequence 179 completed in 6.0 sec with 1 models; lDDT=77.904\n",
      "Sequence 180 completed in 6.2 sec with 1 models; lDDT=74.996\n",
      "Sequence 181 completed in 5.8 sec with 1 models; lDDT=68.616\n",
      "Sequence 182 completed in 6.0 sec with 1 models; lDDT=70.576\n",
      "Sequence 183 completed in 6.2 sec with 1 models; lDDT=67.084\n",
      "Sequence 184 completed in 5.7 sec with 1 models; lDDT=75.993\n",
      "Sequence 185 completed in 5.7 sec with 1 models; lDDT=65.630\n",
      "Sequence 186 completed in 6.0 sec with 1 models; lDDT=89.625\n",
      "Sequence 187 completed in 6.0 sec with 1 models; lDDT=67.647\n",
      "Sequence 188 completed in 5.8 sec with 1 models; lDDT=92.827\n",
      "Sequence 189 completed in 5.8 sec with 1 models; lDDT=67.785\n",
      "Sequence 190 completed in 5.9 sec with 1 models; lDDT=69.173\n",
      "Sequence 191 completed in 5.8 sec with 1 models; lDDT=92.244\n",
      "Sequence 192 completed in 6.0 sec with 1 models; lDDT=64.369\n",
      "Sequence 193 completed in 6.1 sec with 1 models; lDDT=68.365\n",
      "Sequence 194 completed in 6.4 sec with 1 models; lDDT=69.867\n",
      "Sequence 195 completed in 5.5 sec with 1 models; lDDT=68.669\n",
      "Sequence 196 completed in 6.2 sec with 1 models; lDDT=76.387\n",
      "Sequence 197 completed in 5.7 sec with 1 models; lDDT=68.029\n",
      "Sequence 198 completed in 5.7 sec with 1 models; lDDT=60.609\n",
      "Sequence 199 completed in 6.2 sec with 1 models; lDDT=69.170\n",
      "Sequence 200 completed in 5.8 sec with 1 models; lDDT=71.201\n",
      "Sequence 201 completed in 5.8 sec with 1 models; lDDT=63.593\n",
      "Sequence 202 completed in 6.2 sec with 1 models; lDDT=61.830\n",
      "Sequence 203 completed in 5.5 sec with 1 models; lDDT=49.379\n",
      "Sequence 204 completed in 6.4 sec with 1 models; lDDT=54.595\n",
      "Sequence 205 completed in 5.7 sec with 1 models; lDDT=69.452\n",
      "Sequence 206 completed in 7.7 sec with 1 models; lDDT=55.903\n",
      "Sequence 207 completed in 5.5 sec with 1 models; lDDT=74.364\n",
      "Sequence 208 completed in 5.6 sec with 1 models; lDDT=72.202\n",
      "Sequence 209 completed in 5.5 sec with 1 models; lDDT=61.684\n",
      "Sequence 210 completed in 5.6 sec with 1 models; lDDT=93.975\n",
      "Sequence 211 completed in 5.7 sec with 1 models; lDDT=49.834\n",
      "Sequence 212 completed in 5.8 sec with 1 models; lDDT=91.604\n",
      "Sequence 213 completed in 5.8 sec with 1 models; lDDT=93.163\n",
      "Sequence 214 completed in 6.1 sec with 1 models; lDDT=93.656\n",
      "Sequence 215 completed in 5.8 sec with 1 models; lDDT=71.743\n",
      "Sequence 216 completed in 6.0 sec with 1 models; lDDT=66.843\n",
      "Sequence 217 completed in 5.9 sec with 1 models; lDDT=53.164\n",
      "Sequence 218 completed in 5.8 sec with 1 models; lDDT=57.833\n",
      "Sequence 219 completed in 5.9 sec with 1 models; lDDT=55.086\n",
      "Sequence 220 completed in 5.8 sec with 1 models; lDDT=67.210\n",
      "Sequence 221 completed in 6.1 sec with 1 models; lDDT=64.854\n",
      "Sequence 222 completed in 6.1 sec with 1 models; lDDT=69.901\n",
      "Sequence 223 completed in 5.7 sec with 1 models; lDDT=62.660\n",
      "Sequence 224 completed in 6.1 sec with 1 models; lDDT=61.234\n",
      "Sequence 225 completed in 6.1 sec with 1 models; lDDT=69.961\n",
      "Sequence 226 completed in 5.9 sec with 1 models; lDDT=69.790\n",
      "Sequence 227 completed in 5.6 sec with 1 models; lDDT=68.718\n",
      "Sequence 228 completed in 6.0 sec with 1 models; lDDT=62.568\n",
      "Sequence 229 completed in 6.0 sec with 1 models; lDDT=64.277\n",
      "Sequence 230 completed in 5.7 sec with 1 models; lDDT=63.106\n",
      "Sequence 231 completed in 6.0 sec with 1 models; lDDT=89.026\n",
      "Sequence 232 completed in 6.3 sec with 1 models; lDDT=60.865\n",
      "Sequence 233 completed in 5.8 sec with 1 models; lDDT=69.605\n",
      "Sequence 234 completed in 5.9 sec with 1 models; lDDT=88.238\n",
      "Sequence 235 completed in 5.7 sec with 1 models; lDDT=59.980\n",
      "Sequence 236 completed in 6.1 sec with 1 models; lDDT=56.793\n",
      "Sequence 237 completed in 5.9 sec with 1 models; lDDT=63.182\n",
      "Sequence 238 completed in 5.9 sec with 1 models; lDDT=59.632\n",
      "Sequence 239 completed in 5.8 sec with 1 models; lDDT=56.324\n",
      "Sequence 240 completed in 5.8 sec with 1 models; lDDT=88.795\n",
      "Sequence 241 completed in 5.9 sec with 1 models; lDDT=86.651\n",
      "Sequence 242 completed in 6.1 sec with 1 models; lDDT=90.509\n",
      "Sequence 243 completed in 5.7 sec with 1 models; lDDT=90.118\n",
      "Sequence 244 completed in 6.2 sec with 1 models; lDDT=90.264\n",
      "Sequence 245 completed in 7.7 sec with 1 models; lDDT=86.059\n",
      "Sequence 246 completed in 5.5 sec with 1 models; lDDT=89.514\n",
      "Sequence 247 completed in 5.7 sec with 1 models; lDDT=89.355\n",
      "Sequence 248 completed in 5.6 sec with 1 models; lDDT=92.640\n",
      "Sequence 249 completed in 5.6 sec with 1 models; lDDT=72.070\n",
      "Sequence 250 completed in 5.6 sec with 1 models; lDDT=92.887\n",
      "Sequence 251 completed in 5.5 sec with 1 models; lDDT=88.404\n",
      "Sequence 252 completed in 5.9 sec with 1 models; lDDT=63.043\n",
      "Sequence 253 completed in 6.2 sec with 1 models; lDDT=56.826\n",
      "Sequence 254 completed in 6.0 sec with 1 models; lDDT=69.005\n",
      "Sequence 255 completed in 5.7 sec with 1 models; lDDT=72.407\n",
      "Sequence 256 completed in 6.0 sec with 1 models; lDDT=72.003\n",
      "Sequence 257 completed in 6.0 sec with 1 models; lDDT=65.387\n",
      "Sequence 258 completed in 5.7 sec with 1 models; lDDT=68.146\n",
      "Sequence 259 completed in 5.8 sec with 1 models; lDDT=85.377\n",
      "Sequence 260 completed in 6.0 sec with 1 models; lDDT=87.755\n",
      "Sequence 261 completed in 6.0 sec with 1 models; lDDT=76.791\n",
      "Sequence 262 completed in 6.0 sec with 1 models; lDDT=91.357\n",
      "Sequence 263 completed in 6.0 sec with 1 models; lDDT=93.722\n",
      "Sequence 264 completed in 5.9 sec with 1 models; lDDT=48.743\n",
      "Sequence 265 completed in 6.1 sec with 1 models; lDDT=55.772\n",
      "Sequence 266 completed in 5.9 sec with 1 models; lDDT=74.420\n",
      "Sequence 267 completed in 5.7 sec with 1 models; lDDT=50.785\n",
      "Sequence 268 completed in 6.2 sec with 1 models; lDDT=87.980\n",
      "Sequence 269 completed in 5.8 sec with 1 models; lDDT=85.098\n",
      "Sequence 270 completed in 5.8 sec with 1 models; lDDT=79.133"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n",
      "warning: Linking two modules of different target triples: 'LLVMDialectModule' is 'nvptx64-nvidia-gpulibs' whereas '' is 'nvptx64-nvidia-cuda'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequence 271 completed in 5.8 sec with 1 models; lDDT=58.474\n",
      "Sequence 272 completed in 5.8 sec with 1 models; lDDT=59.380\n",
      "Sequence 273 completed in 6.1 sec with 1 models; lDDT=53.231\n",
      "Sequence 274 completed in 5.8 sec with 1 models; lDDT=61.305\n",
      "Sequence 275 completed in 6.1 sec with 1 models; lDDT=61.167\n",
      "Sequence 276 completed in 6.1 sec with 1 models; lDDT=86.209\n",
      "Sequence 277 completed in 5.9 sec with 1 models; lDDT=88.502\n",
      "Sequence 278 completed in 6.0 sec with 1 models; lDDT=89.515\n",
      "Sequence 279 completed in 5.6 sec with 1 models; lDDT=89.900\n",
      "Sequence 280 completed in 6.2 sec with 1 models; lDDT=92.032\n",
      "Sequence 281 completed in 5.9 sec with 1 models; lDDT=91.882\n",
      "Sequence 282 completed in 5.6 sec with 1 models; lDDT=90.699\n",
      "Sequence 283 completed in 5.9 sec with 1 models; lDDT=77.368\n",
      "Sequence 284 completed in 6.1 sec with 1 models; lDDT=85.579\n",
      "Sequence 285 completed in 7.8 sec with 1 models; lDDT=83.127\n",
      "Sequence 286 completed in 5.7 sec with 1 models; lDDT=72.449\n",
      "Sequence 287 completed in 5.6 sec with 1 models; lDDT=90.085\n",
      "Sequence 288 completed in 5.6 sec with 1 models; lDDT=82.381\n",
      "Sequence 289 completed in 5.8 sec with 1 models; lDDT=74.461\n",
      "Sequence 290 completed in 5.6 sec with 1 models; lDDT=74.043\n",
      "Sequence 291 completed in 5.7 sec with 1 models; lDDT=91.412\n",
      "Sequence 292 completed in 5.6 sec with 1 models; lDDT=70.694\n",
      "Sequence 293 completed in 5.7 sec with 1 models; lDDT=94.040\n",
      "Sequence 294 completed in 6.2 sec with 1 models; lDDT=56.444\n",
      "Sequence 295 completed in 5.9 sec with 1 models; lDDT=65.500\n",
      "Sequence 296 completed in 5.8 sec with 1 models; lDDT=58.205\n",
      "Sequence 297 completed in 6.0 sec with 1 models; lDDT=41.496\n",
      "Sequence 298 completed in 6.0 sec with 1 models; lDDT=65.473\n",
      "Sequence 299 completed in 5.8 sec with 1 models; lDDT=67.353\n",
      "Sequence 300 completed in 6.0 sec with 1 models; lDDT=58.504\n",
      "Sequence 301 completed in 5.9 sec with 1 models; lDDT=57.057\n",
      "Sequence 302 completed in 6.0 sec with 1 models; lDDT=56.870\n",
      "Sequence 303 completed in 6.1 sec with 1 models; lDDT=56.602\n",
      "Sequence 304 completed in 5.8 sec with 1 models; lDDT=66.498\n",
      "Sequence 305 completed in 5.8 sec with 1 models; lDDT=56.330\n",
      "Sequence 306 completed in 6.0 sec with 1 models; lDDT=58.880\n",
      "Sequence 307 completed in 5.9 sec with 1 models; lDDT=71.390\n",
      "Sequence 308 completed in 5.9 sec with 1 models; lDDT=70.120\n",
      "Sequence 309 completed in 5.7 sec with 1 models; lDDT=69.030\n",
      "Sequence 310 completed in 6.0 sec with 1 models; lDDT=76.678\n",
      "Sequence 311 completed in 5.7 sec with 1 models; lDDT=72.038\n",
      "Sequence 312 completed in 6.1 sec with 1 models; lDDT=65.387\n",
      "Sequence 313 completed in 6.2 sec with 1 models; lDDT=71.251\n",
      "Sequence 314 completed in 5.8 sec with 1 models; lDDT=74.513\n",
      "Sequence 315 completed in 5.8 sec with 1 models; lDDT=68.610\n",
      "Sequence 316 completed in 5.9 sec with 1 models; lDDT=72.288\n",
      "Sequence 317 completed in 6.0 sec with 1 models; lDDT=69.342\n",
      "Sequence 318 completed in 5.7 sec with 1 models; lDDT=65.498\n",
      "Sequence 319 completed in 6.0 sec with 1 models; lDDT=76.962\n",
      "Sequence 320 completed in 5.7 sec with 1 models; lDDT=67.427\n",
      "Sequence 321 completed in 6.0 sec with 1 models; lDDT=88.328\n",
      "Sequence 322 completed in 6.0 sec with 1 models; lDDT=74.718\n",
      "Sequence 323 completed in 5.9 sec with 1 models; lDDT=72.932\n",
      "Sequence 324 completed in 6.1 sec with 1 models; lDDT=90.146\n",
      "Sequence 325 completed in 7.8 sec with 1 models; lDDT=53.206\n",
      "Sequence 326 completed in 5.8 sec with 1 models; lDDT=46.058\n",
      "Sequence 327 completed in 5.8 sec with 1 models; lDDT=50.607\n",
      "Sequence 328 completed in 5.8 sec with 1 models; lDDT=51.956\n",
      "Sequence 329 completed in 5.8 sec with 1 models; lDDT=89.895\n",
      "Sequence 330 completed in 5.8 sec with 1 models; lDDT=93.152\n",
      "Sequence 331 completed in 5.7 sec with 1 models; lDDT=79.490\n",
      "Sequence 332 completed in 5.8 sec with 1 models; lDDT=86.442\n",
      "Sequence 333 completed in 5.7 sec with 1 models; lDDT=82.213\n",
      "Sequence 334 completed in 5.4 sec with 1 models; lDDT=78.288\n",
      "Sequence 335 completed in 5.7 sec with 1 models; lDDT=70.230\n",
      "Sequence 336 completed in 62.1 sec with 1 models; lDDT=65.868\n",
      "Sequence 337 completed in 6.2 sec with 1 models; lDDT=78.616\n",
      "Sequence 338 completed in 5.6 sec with 1 models; lDDT=80.514\n",
      "Sequence 339 completed in 5.8 sec with 1 models; lDDT=87.078\n",
      "Sequence 340 completed in 6.1 sec with 1 models; lDDT=79.342\n",
      "Sequence 341 completed in 6.0 sec with 1 models; lDDT=67.968\n",
      "Sequence 342 completed in 5.8 sec with 1 models; lDDT=77.655\n",
      "Sequence 343 completed in 5.9 sec with 1 models; lDDT=86.879\n",
      "Sequence 344 completed in 6.0 sec with 1 models; lDDT=67.821\n",
      "Sequence 345 completed in 5.8 sec with 1 models; lDDT=70.434\n",
      "Sequence 346 completed in 6.0 sec with 1 models; lDDT=73.061\n",
      "Sequence 347 completed in 5.9 sec with 1 models; lDDT=73.780\n",
      "Sequence 348 completed in 5.9 sec with 1 models; lDDT=85.457\n",
      "Sequence 349 completed in 6.2 sec with 1 models; lDDT=71.131\n",
      "Sequence 350 completed in 5.8 sec with 1 models; lDDT=82.401\n",
      "Sequence 351 completed in 6.0 sec with 1 models; lDDT=87.581\n",
      "Sequence 352 completed in 6.0 sec with 1 models; lDDT=80.071\n",
      "Sequence 353 completed in 6.0 sec with 1 models; lDDT=80.159\n",
      "Sequence 354 completed in 5.9 sec with 1 models; lDDT=60.868\n",
      "Sequence 355 completed in 5.6 sec with 1 models; lDDT=68.032\n",
      "Sequence 356 completed in 5.6 sec with 1 models; lDDT=80.201\n",
      "Sequence 357 completed in 6.3 sec with 1 models; lDDT=60.260\n",
      "Sequence 358 completed in 6.0 sec with 1 models; lDDT=66.619\n",
      "Sequence 359 completed in 5.9 sec with 1 models; lDDT=59.286\n",
      "Sequence 360 completed in 5.7 sec with 1 models; lDDT=80.859\n",
      "Sequence 361 completed in 5.8 sec with 1 models; lDDT=87.536\n",
      "Sequence 362 completed in 6.1 sec with 1 models; lDDT=73.299\n",
      "Sequence 363 completed in 5.9 sec with 1 models; lDDT=87.083\n",
      "Sequence 364 completed in 5.8 sec with 1 models; lDDT=88.865\n",
      "Sequence 365 completed in 5.7 sec with 1 models; lDDT=85.092\n",
      "Sequence 366 completed in 6.3 sec with 1 models; lDDT=83.913\n",
      "Sequence 367 completed in 6.0 sec with 1 models; lDDT=84.417\n",
      "Sequence 368 completed in 5.7 sec with 1 models; lDDT=84.787\n",
      "Sequence 369 completed in 6.1 sec with 1 models; lDDT=83.747\n",
      "Sequence 370 completed in 8.1 sec with 1 models; lDDT=84.296\n",
      "Sequence 371 completed in 6.1 sec with 1 models; lDDT=83.136\n",
      "Sequence 372 completed in 6.6 sec with 1 models; lDDT=81.065\n",
      "Sequence 373 completed in 5.9 sec with 1 models; lDDT=84.492\n",
      "Sequence 374 completed in 6.1 sec with 1 models; lDDT=76.524\n",
      "Sequence 375 completed in 5.8 sec with 1 models; lDDT=84.072\n",
      "Sequence 376 completed in 6.1 sec with 1 models; lDDT=82.400\n",
      "Sequence 377 completed in 6.3 sec with 1 models; lDDT=83.073\n",
      "Sequence 378 completed in 6.1 sec with 1 models; lDDT=84.738\n",
      "Sequence 379 completed in 6.1 sec with 1 models; lDDT=85.740\n",
      "Sequence 380 completed in 6.1 sec with 1 models; lDDT=84.075\n",
      "Sequence 381 completed in 5.7 sec with 1 models; lDDT=88.358\n",
      "Sequence 382 completed in 5.7 sec with 1 models; lDDT=80.987\n",
      "Sequence 383 completed in 6.0 sec with 1 models; lDDT=81.827\n",
      "Sequence 384 completed in 5.9 sec with 1 models; lDDT=92.918\n",
      "Sequence 385 completed in 6.0 sec with 1 models; lDDT=93.266\n",
      "Sequence 386 completed in 5.7 sec with 1 models; lDDT=87.681\n",
      "Sequence 387 completed in 6.0 sec with 1 models; lDDT=86.116\n",
      "Sequence 388 completed in 5.8 sec with 1 models; lDDT=86.692\n",
      "Sequence 389 completed in 5.9 sec with 1 models; lDDT=88.821\n",
      "Sequence 390 completed in 5.9 sec with 1 models; lDDT=88.696\n",
      "Sequence 391 completed in 5.7 sec with 1 models; lDDT=82.021\n",
      "Sequence 392 completed in 5.7 sec with 1 models; lDDT=88.256\n",
      "Sequence 393 completed in 6.2 sec with 1 models; lDDT=89.580\n",
      "Sequence 394 completed in 6.1 sec with 1 models; lDDT=86.896\n",
      "Sequence 395 completed in 5.6 sec with 1 models; lDDT=83.535\n",
      "Sequence 396 completed in 5.9 sec with 1 models; lDDT=69.916\n",
      "Sequence 397 completed in 5.8 sec with 1 models; lDDT=63.683\n",
      "Sequence 398 completed in 6.0 sec with 1 models; lDDT=58.383\n",
      "Sequence 399 completed in 5.9 sec with 1 models; lDDT=61.919\n",
      "Sequence 400 completed in 5.9 sec with 1 models; lDDT=67.190\n",
      "Sequence 401 completed in 5.7 sec with 1 models; lDDT=67.938\n",
      "Sequence 402 completed in 6.0 sec with 1 models; lDDT=80.693\n",
      "Sequence 403 completed in 6.1 sec with 1 models; lDDT=65.744\n",
      "Sequence 404 completed in 5.8 sec with 1 models; lDDT=69.903\n",
      "Sequence 405 completed in 5.9 sec with 1 models; lDDT=75.261"
     ]
    }
   ],
   "source": [
    "## Setting up AlphaFold2 run\n",
    "\n",
    "AF2_recycles = 3\n",
    "AF2_models = \"4\"  # add other models to this string if needed, i.e. \"3 4 5\"\n",
    "\n",
    "commands_af2 = []\n",
    "cmds_filename_af2 = \"commands_af2\"\n",
    "with open(cmds_filename_af2, \"w\") as file:\n",
    "    for ff in glob.glob(\"*.fasta\"):\n",
    "        commands_af2.append(f\"{PYTHON['af2']} {AF2_script} \"\n",
    "                             f\"--af-nrecycles {AF2_recycles} --af-models {AF2_models} \"\n",
    "                             f\"--fasta {ff} --scorefile {ff.replace('.fasta', '.csv')}\\n\")\n",
    "        file.write(commands_af2[-1])\n",
    "\n",
    "print(\"Example AF2 command:\")\n",
    "print(commands_af2[-1]) \n",
    "\n",
    "with open(cmds_filename_af2, 'r') as file:\n",
    "    for line in file:\n",
    "        with threadpool_limits(limits=1):\n",
    "            p = subprocess.Popen(line, shell=True)\n",
    "            (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98095a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you're done with AF2 and happy with the outputs then mark it as done\n",
    "AF2_DIR = f\"{WDIR}/5.1_2nd_af2\"\n",
    "os.chdir(AF2_DIR)\n",
    "DESIGN_DIR_ligMPNN = f\"{WDIR}/3.1_design_pocket_ligandMPNN\"\n",
    "\n",
    "if not os.path.exists(AF2_DIR+\"/.done\"):\n",
    "    with open(f\"{AF2_DIR}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c770263c",
   "metadata": {},
   "source": [
    "### Analyzing AlphaFold2 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fd05f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all CSV scorefiles into one\n",
    "os.system(\"head -n 1 $(ls all_seq2.csv | shuf -n 1) > scores.csv ; for f in all_seq2.csv ; do tail -n +2 ${f} >> scores.csv ; done\")\n",
    "assert os.path.exists(\"scores.csv\"), \"Could not combine scorefiles\"\n",
    "\n",
    "### Calculating the RMSDs of AF2 predictions relative to the diffusion outputs\n",
    "### Catalytic residue sidechain RMSDs are calculated in the reference PDB has REMARK 666 line present\n",
    "\n",
    "analysis_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/utils/analyze_af2.py --scorefile scores.csv \"\\\n",
    "               f\"--ref_path {DESIGN_DIR_ligMPNN}/good/ --mpnn --params {' '.join(params)}\"\n",
    "\n",
    "p = subprocess.Popen(analysis_cmd, shell=True)\n",
    "(output, err) = p.communicate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512c33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualizing and filtering AF2 results\n",
    "AF2_DIR = f\"{WDIR}/5.1_2nd_af2\"\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "scores_af2 = pd.read_csv(\"scores.sc\", sep=\"\\s+\", header=0)\n",
    "\n",
    "### Filtering AF2 scores based on lddt and rmsd\n",
    "# Define your desired cutoffs here:\n",
    "AF2_filters = {\"lDDT\": [85.0, \">=\"],\n",
    "               \"rmsd\": [1.2, \"<=\"]}\n",
    "\n",
    "scores_af2_filtered = utils.filter_scores(scores_af2, AF2_filters)\n",
    "utils.dump_scorefile(scores_af2_filtered, \"filtered_scores.sc\")\n",
    "\n",
    "## Plotting AF2 scores\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i,k in enumerate(AF2_filters):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.hist(scores_af2[k])\n",
    "    plt.title(k)\n",
    "    plt.xlabel(k)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "utils.plot_score_pairs(scores_af2, \"lDDT\", \"rmsd\", AF2_filters[\"lDDT\"][0], AF2_filters[\"rmsd\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de13ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Copying good predictions to a separate directory\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "if len(scores_af2_filtered) > 0:\n",
    "    os.makedirs(\"good\", exist_ok=True)\n",
    "    good_af2_models = [row[\"Output_PDB\"]+\".pdb\" for idx,row in scores_af2_filtered.iterrows()]\n",
    "    for pdb in good_af2_models:\n",
    "        copy2(pdb, f\"good/{pdb}\")\n",
    "    good_af2_models = glob.glob(f\"{AF2_DIR}/good/*.pdb\")\n",
    "else:\n",
    "    sys.exit(\"No good models to continue this pipeline with\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc2ddec",
   "metadata": {},
   "source": [
    "## 6.1: Final FastRelax with the ligand\n",
    "Relaxing good AF2 models together with the ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268bceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "AF2_DIR = f\"{WDIR}/5.1_2nd_af2\"\n",
    "DESIGN_DIR_ligMPNN = f\"{WDIR}/3.1_design_pocket_ligandMPNN\"\n",
    "assert len(glob.glob(f\"{AF2_DIR}/good/*.pdb\")) > 0, \"No good AF2 models to relax with\"\n",
    "\n",
    "os.chdir(WDIR)\n",
    "RELAX_DIR = f\"{WDIR}/6.1_final_relax\"\n",
    "os.makedirs(RELAX_DIR, exist_ok=True)\n",
    "os.chdir(RELAX_DIR)\n",
    "\n",
    "os.makedirs(RELAX_DIR+\"/logs\", exist_ok=True)\n",
    "\n",
    "## First matching up the AF2 output filenames of step 5 with pocket design filenames from step 3\n",
    "ref_and_model_pairs = []\n",
    "for r in glob.glob(f\"{DESIGN_DIR_ligMPNN}/good/*.pdb\"):\n",
    "    for pdbfile in glob.glob(f\"{AF2_DIR}/good/*.pdb\"):\n",
    "        if os.path.basename(r).replace(\".pdb\", \"_\") in pdbfile:\n",
    "            ref_and_model_pairs.append((r, pdbfile))\n",
    "\n",
    "assert len(ref_and_model_pairs) == len(glob.glob(f\"{AF2_DIR}/good/*.pdb\")), \"Was not able to match all models with reference structures\"\n",
    "\n",
    "\n",
    "## Generating commands for relax jobs\n",
    "### Performing 1 relax iteration on each input structure\n",
    "NSTRUCT = 1\n",
    "\n",
    "commands_relax = []\n",
    "cmds_filename_rlx = \"commands_relax\"\n",
    "with open(cmds_filename_rlx, \"w\") as file:\n",
    "    for r_m in ref_and_model_pairs:\n",
    "        commands_relax.append(f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/design/align_add_ligand_relax.py \"\n",
    "                              f\"--outdir ./ --ligand {LIGAND} --ref_pdb {r_m[0]} \"\n",
    "                              f\"--pdb {r_m[1]} --nstruct {NSTRUCT} \"\n",
    "                              f\"--params {' '.join(params)} > logs/{os.path.basename(r_m[1]).replace('.pdb', '.log')}\\n\")\n",
    "        file.write(commands_relax[-1])\n",
    "\n",
    "print(\"Example design command:\")\n",
    "print(commands_relax[-1])\n",
    "\n",
    "with open(cmds_filename_rlx, 'r') as file:\n",
    "    for line in file:\n",
    "        p = subprocess.Popen(line, shell=True)\n",
    "        (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366ce521",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you're done with final relax and happy with the outputs then mark it as done\n",
    "RELAX_DIR = f\"{WDIR}/6.1_final_relax\"\n",
    "os.chdir(RELAX_DIR)\n",
    "\n",
    "if not os.path.exists(RELAX_DIR+\"/.done\"):\n",
    "    with open(f\"{RELAX_DIR}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d848dbbf",
   "metadata": {},
   "source": [
    "### Analyzing final relaxed structures\n",
    "Filtering them based on the same metrics as was used for the initial design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3744d0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyzing Rosetta designs\n",
    "RELAX_DIR = f\"{WDIR}/6.1_final_relax\"\n",
    "os.chdir(RELAX_DIR)\n",
    "\n",
    "scores = pd.read_csv(\"scorefile.txt\", sep=r\"\\s+\", header=0)\n",
    "\n",
    "filters = {\n",
    " 'nlr_totrms': [1.0, '<='],\n",
    " 'L_SASA': [0.2, '<='],\n",
    " 'score_per_res': [0.0, '<='],\n",
    " 'corrected_ddg': [-50.0, '<='],\n",
    " 'cms_per_atom': [4.8, '>='],\n",
    " 'rmsd_CA_rlx_in': [1.0, \"<=\"]}  # rmsd_CA_rlx_in is rmsd between relaxed structure and AF2 prediction\n",
    "\n",
    "filtered_scores = utils.filter_scores(scores, filters)\n",
    "\n",
    "## Plotting relax scores\n",
    "plt.figure(figsize=(12, 9))\n",
    "for i,k in enumerate(filters):\n",
    "    if k not in scores.keys():\n",
    "        continue\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    plt.hist(scores[k])\n",
    "    plt.title(k)\n",
    "    plt.xlabel(k)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### Copying good designs over to a new directory\n",
    "if len(filtered_scores) > 0:\n",
    "    os.makedirs(f\"{RELAX_DIR}/good\", exist_ok=True)\n",
    "    for idx, row in filtered_scores.iterrows():\n",
    "        copy2(row[\"description\"]+\".pdb\", \"good/\"+row[\"description\"]+\".pdb\")\n",
    "else:\n",
    "    print(\"No good designs created, bummer...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e448ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(filtered_scores) > 0:\n",
    "    print(f\"CONGRATULATIONS! You have successfully designed {len(filtered_scores)} proteins against ligand {LIGAND}\")\n",
    "    print(\"You can find the design models in the directory:\\n\"\n",
    "          f\"    {RELAX_DIR}/good\")\n",
    "    print(\"\\nIt is advised you manually inspect them before ordering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def0617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPAINT_DIR = f\"{WDIR}/inpainting/0_inpainting\"\n",
    "assert len(glob.glob(INPAINT_DIR+\"/alignment/*.pdb\")) > 0, \"No good backbones found!\"\n",
    "\n",
    "os.chdir(WDIR)\n",
    "\n",
    "MPNN_DIR = f\"{WDIR}/inpainting/1_proteinmpnn\"\n",
    "os.makedirs(MPNN_DIR, exist_ok=True)\n",
    "os.chdir(MPNN_DIR)\n",
    "\n",
    "### Parsing diffusion output TRB files to extract fixed motif residues\n",
    "## These residues will not be redesigned with proteinMPNN\n",
    "mask_json_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/design/make_maskdict_from_trb.py \"\\\n",
    "                f\"--out masked_pos.jsonl --trb {' '.join(glob.glob(INPAINT_DIR+'/*.trb'))}\"\n",
    "p = subprocess.Popen(mask_json_cmd, shell=True)\n",
    "(output, err) = p.communicate()\n",
    "\n",
    "assert os.path.exists(\"masked_pos.jsonl\"), \"Failed to create masked positions JSONL file\"\n",
    "\n",
    "\n",
    "### Setting up proteinMPNN run commands\n",
    "## We're doing design with 3 temperatures, and 5 sequences each.\n",
    "## This usually gives decent success with designable backbones. For more complicated cases consider doing >100 sequences.\n",
    "\n",
    "MPNN_temperatures = [0.1, 0.2, 0.3]\n",
    "MPNN_outputs_per_temperature = 5\n",
    "MPNN_omit_AAs = \"CM\"\n",
    "\n",
    "commands_mpnn = []\n",
    "cmds_filename_mpnn = \"commands_mpnn\"\n",
    "with open(cmds_filename_mpnn, \"w\") as file:\n",
    "    for T in MPNN_temperatures:\n",
    "        for f in glob.glob(INPAINT_DIR+\"/alignment/*.pdb\"):\n",
    "            commands_mpnn.append(f\"{PYTHON['proteinMPNN']} {proteinMPNN_script} \"\n",
    "                                 f\"--model_type protein_mpnn --ligand_mpnn_use_atom_context 0 \"\n",
    "                                 \"--fixed_residues_multi masked_pos.jsonl --out_folder ./ \"\n",
    "                                 f\"--number_of_batches {MPNN_outputs_per_temperature} --temperature {T} \"\n",
    "                                 f\"--omit_AA {MPNN_omit_AAs} --pdb_path {f} \"\n",
    "                                 f\"--checkpoint_protein_mpnn {SCRIPT_DIR}/lib/LigandMPNN/model_params/proteinmpnn_v_48_020.pt\\n\")\n",
    "            file.write(commands_mpnn[-1])\n",
    "\n",
    "            \n",
    "            \n",
    "print(f\"{len(commands_mpnn)} MPNN jobs to run\")\n",
    "print(\"Example MPNN command:\")\n",
    "print(commands_mpnn[-1])\n",
    "\n",
    "### Running proteinMPNN with Slurm.\n",
    "### Grouping jobs with 10 commands per one array job.\n",
    "\n",
    "submit_script = \"submit_mpnn.sh\"\n",
    "utils.create_slurm_submit_script(filename=submit_script, name=\"1_proteinmpnn\", mem=\"4g\", \n",
    "                                 N_cores=1, time=\"0:15:00\", email=EMAIL, array=len(commands_mpnn),\n",
    "                                 array_commandfile=cmds_filename_mpnn, group=10)\n",
    "\n",
    "if not os.path.exists(MPNN_DIR+\"/.done\"):\n",
    "    p = subprocess.Popen(['sbatch', submit_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ab8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you're done with MPNN and happy with the outputs then mark it as done\n",
    "MPNN_DIR = f\"{WDIR}/inpainting/1_proteinmpnn\"\n",
    "os.chdir(MPNN_DIR)\n",
    "\n",
    "if not os.path.exists(MPNN_DIR+\"/.done\"):\n",
    "    with open(f\"{MPNN_DIR}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f19d9ce",
   "metadata": {},
   "source": [
    "## 2: Running AlphaFold2\n",
    "Performing AF2 single sequence predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WDIR)\n",
    "assert len(glob.glob(MPNN_DIR+\"/seqs/*.fa\")) > 0, \"No MPNN outputs to run AF2 on\"\n",
    "\n",
    "AF2_DIR = f\"{WDIR}/inpainting/2_af2\"\n",
    "os.makedirs(AF2_DIR, exist_ok=True)\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "### First collecting MPNN outputs and creating FASTA files for AF2 input\n",
    "mpnn_fasta = utils.parse_fasta_files(glob.glob(f\"{MPNN_DIR}/seqs/*.fa\"))\n",
    "mpnn_fasta = {k: seq.strip() for k, seq in mpnn_fasta.items() if \"model_path\" not in k}  # excluding the diffused poly-A sequence\n",
    "# Giving sequences unique names based on input PDB name, temperature, and sequence identifier\n",
    "mpnn_fasta = {k.split(\",\")[0]+\"_\"+k.split(\",\")[2].replace(\" T=\", \"T\")+\"_0_\"+k.split(\",\")[1].replace(\" id=\", \"\"): seq for k, seq in mpnn_fasta.items()}\n",
    "\n",
    "print(f\"A total on {len(mpnn_fasta)} sequences will be predicted.\")\n",
    "\n",
    "## Splitting the MPNN sequences based on length\n",
    "## and grouping them in smaller batches for each AF2 job\n",
    "## Use group size of >40 when running on GPU. Also depends on how many sequences and resources you have.\n",
    "SEQUENCES_PER_AF2_JOB = 4  # CPU\n",
    "if USE_GPU_for_AF2 is True:\n",
    "    SEQUENCES_PER_AF2_JOB = 100  # GPU\n",
    "mpnn_fasta_split = utils.split_fasta_based_on_length(mpnn_fasta, SEQUENCES_PER_AF2_JOB, write_files=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d5b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up AlphaFold2 run\n",
    "\n",
    "AF2_recycles = 3\n",
    "AF2_models = \"4\"  # add other models to this string if needed, i.e. \"3 4 5\"\n",
    "\n",
    "commands_af2 = []\n",
    "cmds_filename_af2 = \"commands_af2\"\n",
    "with open(cmds_filename_af2, \"w\") as file:\n",
    "    for ff in glob.glob(\"*.fasta\"):\n",
    "        commands_af2.append(f\"{PYTHON['af2']} {AF2_script} \"\n",
    "                             f\"--af-nrecycles {AF2_recycles} --af-models {AF2_models} \"\n",
    "                             f\"--fasta {ff} --scorefile {ff.replace('.fasta', '.csv')}\\n\")\n",
    "        file.write(commands_af2[-1])\n",
    "\n",
    "print(\"Example AF2 command:\")\n",
    "print(commands_af2[-1])\n",
    "\n",
    "### Running AF2 with Slurm.\n",
    "### Running jobs on the CPU. It takes ~10 minutes per sequence\n",
    "### \n",
    "\n",
    "submit_script = \"submit_af2.sh\"\n",
    "if USE_GPU_for_AF2 is True:\n",
    "    utils.create_slurm_submit_script(filename=submit_script, name=\"2_af2\", mem=\"6g\", \n",
    "                                     N_cores=2, gpu=True, gres=\"gpu:rtx2080:1\", time=\"00:12:00\", email=EMAIL, array=len(commands_af2),\n",
    "                                     array_commandfile=cmds_filename_af2)\n",
    "else:\n",
    "    utils.create_slurm_submit_script(filename=submit_script, name=\"2_af2\", mem=\"6g\", \n",
    "                                     N_cores=4, time=\"01:00:00\", email=EMAIL, array=len(commands_af2),\n",
    "                                     array_commandfile=cmds_filename_af2)\n",
    "\n",
    "if not os.path.exists(AF2_DIR+\"/.done\"):\n",
    "    p = subprocess.Popen(['sbatch', submit_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0c7b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you're done with AF2 and happy with the outputs then mark it as done\n",
    "AF2_DIR = f\"{WDIR}/inpainting/2_af2\"\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "if not os.path.exists(AF2_DIR+\"/.done\"):\n",
    "    with open(f\"{AF2_DIR}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184907b7",
   "metadata": {},
   "source": [
    "### Analyzing AlphaFold2 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPAINT_DIR = f\"{WDIR}/inpainting/0_inpainting\"\n",
    "\n",
    "# Combining all CSV scorefiles into one\n",
    "os.system(\"head -n 1 $(ls *aa*.csv | shuf -n 1) > scores.csv ; for f in *aa*.csv ; do tail -n +2 ${f} >> scores.csv ; done\")\n",
    "assert os.path.exists(\"scores.csv\"), \"Could not combine scorefiles\"\n",
    "\n",
    "### Calculating the RMSDs of AF2 predictions relative to the diffusion outputs\n",
    "### Catalytic residue sidechain RMSDs are calculated in the reference PDB has REMARK 666 line present\n",
    "\n",
    "analysis_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/utils/analyze_af2.py --scorefile scores.csv \"\\\n",
    "               f\"--ref_path {INPAINT_DIR}/alignment/ --mpnn --params {' '.join(params)}\"\n",
    "\n",
    "p = subprocess.Popen(analysis_cmd, shell=True)\n",
    "(output, err) = p.communicate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e5a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualizing and filtering AF2 predictions\n",
    "AF2_DIR = f\"{WDIR}/inpainting/2_af2\"\n",
    "\n",
    "scores_af2 = pd.read_csv(\"scores.sc\", sep=\"\\s+\", header=0)\n",
    "\n",
    "### Filtering AF2 scores based on lddt and rmsd\n",
    "# Define your desired cutoffs here:\n",
    "AF2_filters = {\"lDDT\": [85.0, \">=\"],\n",
    "               \"rmsd\": [1.5, \"<=\"],\n",
    "               \"rmsd_SR1\": [2.0, \"<=\"]}  # 1st catalytic residue sc-rmsd\n",
    "\n",
    "scores_af2_filtered = utils.filter_scores(scores_af2, AF2_filters)\n",
    "utils.dump_scorefile(scores_af2_filtered, \"filtered_scores.sc\")\n",
    "\n",
    "## Plotting AF2 scores\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i,k in enumerate(AF2_filters):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.hist(scores_af2[k])\n",
    "    plt.title(k)\n",
    "    plt.xlabel(k)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "utils.plot_score_pairs(scores_af2, \"lDDT\", \"rmsd\", AF2_filters[\"lDDT\"][0], AF2_filters[\"rmsd\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ac5be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Copying good predictions to a separate directory\n",
    "os.chdir(AF2_DIR)\n",
    "INPAINT_DIR = f\"{WDIR}/inpainting/0_inpainting\"\n",
    "\n",
    "if len(scores_af2_filtered) > 0:\n",
    "    os.makedirs(\"good\", exist_ok=True)\n",
    "    good_af2_models = [row[\"Output_PDB\"]+\".pdb\" for idx,row in scores_af2_filtered.iterrows()]\n",
    "    for pdb in good_af2_models:\n",
    "        copy2(pdb, f\"good/{pdb}\")\n",
    "    good_af2_models = glob.glob(f\"{AF2_DIR}/good/*.pdb\")\n",
    "else:\n",
    "    sys.exit(\"No good models to continue this pipeline with\")\n",
    "\n",
    "os.chdir(f\"{AF2_DIR}/good\")\n",
    "\n",
    "\n",
    "### Aligning the ligand back into the AF2 predictions.\n",
    "### This is done by aligning the AF2 model to diffusion output and copying over the ligand using PyRosetta.\n",
    "### --fix_catres option will re-adjust the rotamer and tautomer of \n",
    "### any catalytic residue to be the same as in the reference model.\n",
    "\n",
    "align_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/utils/place_ligand_after_af2.py \"\\\n",
    "            f\"--outdir with_heme --params {' '.join(params)} --fix_catres \"\\\n",
    "            f\"--pdb {' '.join(good_af2_models)} \"\\\n",
    "            f\"--ref {' '.join(glob.glob(INPAINT_DIR+'/alignment/*.pdb'))}\"\n",
    "\n",
    "p = subprocess.Popen(align_cmd, shell=True)\n",
    "(output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9ad826",
   "metadata": {},
   "source": [
    "## 3.1: Performing binding site design with LigandMPNN / FastRelax\n",
    "<a id='ligmpnn_fr'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ad0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Setting up design directory and commands\n",
    "os.chdir(WDIR)\n",
    "DESIGN_DIR_ligMPNN = f\"{WDIR}/inpainting/3.1_design_pocket_ligandMPNN\"\n",
    "os.makedirs(DESIGN_DIR_ligMPNN, exist_ok=True)\n",
    "os.chdir(DESIGN_DIR_ligMPNN)\n",
    "\n",
    "AF2_DIR = f\"{WDIR}/inpainting/2_af2\"  # change this if you want to run it on inpainting outputs\n",
    "os.makedirs(DESIGN_DIR_ligMPNN+\"/logs\", exist_ok=True)\n",
    "\n",
    "### Performing 5 design iterations on each input structure\n",
    "NSTRUCT = 10\n",
    "cstfile = f\"{SCRIPT_DIR}/theozyme/HBA/HBA_CYS_UPO.cst\"\n",
    "\n",
    "commands_design = []\n",
    "cmds_filename_des = \"commands_design\"\n",
    "with open(cmds_filename_des, \"w\") as file:\n",
    "    for pdb in glob.glob(f\"{AF2_DIR}/good/with_heme/*.pdb\"):\n",
    "        commands_design.append(f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/design/heme_pocket_ligMPNN.py \"\n",
    "                             f\"--pdb {pdb} --nstruct {NSTRUCT} \"\n",
    "                             f\"--scoring {SCRIPT_DIR}/scripts/design/scoring/heme_scoring.py \"\n",
    "                             f\"--params {' '.join(params)} --cstfile {cstfile} > logs/{os.path.basename(pdb).replace('.pdb', '.log')}\\n\")\n",
    "        file.write(commands_design[-1])\n",
    "\n",
    "print(\"Example design command:\")\n",
    "print(commands_design[-1])\n",
    "\n",
    "\n",
    "### Running design jobs with Slurm.\n",
    "submit_script = \"submit_design.sh\"\n",
    "utils.create_slurm_submit_script(filename=submit_script, name=\"3.1_design_pocket_ligMPNN\", mem=\"4g\", \n",
    "                                 N_cores=1, time=\"3:00:00\", email=EMAIL, array=len(commands_design),\n",
    "                                 array_commandfile=cmds_filename_des)\n",
    "\n",
    "if not os.path.exists(DESIGN_DIR_ligMPNN+\"/.done\"):\n",
    "    p = subprocess.Popen(['sbatch', submit_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77afc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "DESIGN_DIR_ligMPNN = f\"{WDIR}/inpainting/3.1_design_pocket_ligandMPNN\"\n",
    "os.chdir(DESIGN_DIR_ligMPNN)\n",
    "\n",
    "## If you're done with design and happy with the outputs then mark it as done\n",
    "if not os.path.exists(DESIGN_DIR_ligMPNN+\"/.done\"):\n",
    "    with open(f\"{DESIGN_DIR_ligMPNN}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf6697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Analyzing Rosetta designs\n",
    "scores = pd.read_csv(\"scorefile.txt\", sep=\"\\s+\", header=0)\n",
    "\n",
    "filters = {'all_cst': [1.0, '<='],\n",
    " 'nlr_SR1_rms': [0.8, '<='],\n",
    " 'nlr_totrms': [1.0, '<='],\n",
    " 'L_SASA': [0.2, '<='],\n",
    " 'COO_hbond': [1.0, '='],\n",
    " 'heme_angle_wrst': [80.0, '>='],\n",
    " 'score_per_res': [0.0, '<='],\n",
    " 'corrected_ddg': [-50.0, '<='],\n",
    " 'cms_per_atom': [4.8, '>=']}\n",
    "\n",
    "filtered_scores = utils.filter_scores(scores, filters)\n",
    "\n",
    "## Plotting design scores\n",
    "plt.figure(figsize=(12, 9))\n",
    "for i,k in enumerate(filters):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.hist(scores[k])\n",
    "    plt.title(k)\n",
    "    plt.xlabel(k)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### Copying good designs over to a new directory\n",
    "if len(filtered_scores) > 0:\n",
    "    os.makedirs(f\"{DESIGN_DIR_ligMPNN}/good\", exist_ok=True)\n",
    "    for idx, row in filtered_scores.iterrows():\n",
    "        copy2(row[\"description\"]+\".pdb\", \"good/\"+row[\"description\"]+\".pdb\")\n",
    "else:\n",
    "    print(\"No good designs created, bummer...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56c82b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8397b389",
   "metadata": {},
   "source": [
    "## 4.1 Performing ligandMPNN redesign on the 2nd layer residues\n",
    "\n",
    "Resampling residues that are not in the pocket, but also not very far from the pocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac14eee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if len(glob.glob(DESIGN_DIR_ligMPNN+'/good/*.pdb')) == 0:\n",
    "    sys.exit(\"No designs to run 2nd MPNN on.\")\n",
    "\n",
    "os.chdir(WDIR)\n",
    "DESIGN_DIR_2nd_mpnn = f\"{WDIR}/inpainting/4.1_2nd_mpnn\"\n",
    "os.makedirs(DESIGN_DIR_2nd_mpnn, exist_ok=True)\n",
    "os.chdir(DESIGN_DIR_2nd_mpnn)\n",
    "\n",
    "### Making a JSON file specifiying designable positions for each structure.\n",
    "### Will also make non-pocket ALA positions as designable.\n",
    "### This is to fix any surface ALA-patches that previous MPNN may have introduced.\n",
    "\n",
    "make_json_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/design/setup_ligand_mpnn_2nd_layer.py \"\\\n",
    "                f\"--params {' '.join(params)} --ligand {LIGAND} --output_path parsed_pdbs_lig.jsonl \"\\\n",
    "                 \"--output_path masked_pos.jsonl --dist_bb 6.0 --dist_sc 5.0 \"\\\n",
    "                f\"--pdb {' '.join(glob.glob(DESIGN_DIR_ligMPNN+'/good/*.pdb'))}\"\n",
    "\n",
    "p = subprocess.Popen(make_json_cmd, shell=True)\n",
    "(output, err) = p.communicate()\n",
    "\n",
    "if not os.path.exists(\"masked_pos.jsonl\"):\n",
    "    sys.exit()\n",
    "\n",
    "### Setting up ligandMPNN run commands\n",
    "## We're doing design with 2 temperatures (more conservative than before), and 5 sequences each.\n",
    "\n",
    "MPNN_temperatures = [0.1, 0.2]\n",
    "MPNN_outputs_per_temperature = 5\n",
    "MPNN_omit_AAs = \"CM\"\n",
    "\n",
    "commands_mpnn = []\n",
    "cmds_filename_mpnn = \"commands_mpnn\"\n",
    "with open(cmds_filename_mpnn, \"w\") as file:\n",
    "    for T in MPNN_temperatures:\n",
    "        for f in glob.glob(DESIGN_DIR_ligMPNN+'/good/*.pdb'):\n",
    "            commands_mpnn.append(f\"{PYTHON['proteinMPNN']} {proteinMPNN_script} \"\n",
    "                                 f\"--model_type ligand_mpnn --ligand_mpnn_use_atom_context 1 \"\n",
    "                                 \"--fixed_residues_multi masked_pos.jsonl --out_folder ./ \"\n",
    "                                 f\"--number_of_batches {MPNN_outputs_per_temperature} --temperature {T} \"\n",
    "                                 f\"--omit_AA {MPNN_omit_AAs} --pdb_path {f} \"\n",
    "                                 f\"--checkpoint_protein_mpnn {SCRIPT_DIR}/lib/LigandMPNN/model_params/ligandmpnn_v_32_010_25.pt\\n\")\n",
    "            file.write(commands_mpnn[-1])\n",
    "\n",
    "print(\"Example MPNN command:\")\n",
    "print(commands_mpnn[-1])\n",
    "\n",
    "### Running ligandMPNN with Slurm.\n",
    "### Grouping jobs with 10 commands per one array job.\n",
    "\n",
    "submit_script = \"submit_mpnn.sh\"\n",
    "utils.create_slurm_submit_script(filename=submit_script, name=\"4.1_2nd_mpnn\", mem=\"8g\", \n",
    "                                 N_cores=1, time=\"0:15:00\", email=EMAIL, array=len(commands_mpnn),\n",
    "                                 array_commandfile=cmds_filename_mpnn, group=10)\n",
    "\n",
    "if not os.path.exists(DESIGN_DIR_2nd_mpnn+\"/.done\"):\n",
    "    p = subprocess.Popen(['sbatch', submit_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    (output, err) = p.communicate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0233f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "DESIGN_DIR_ligMPNN = f\"{WDIR}/inpainting/3.1_design_pocket_ligandMPNN\"\n",
    "DESIGN_DIR_2nd_mpnn = f\"{WDIR}/inpainting/4.1_2nd_mpnn\"\n",
    "os.chdir(DESIGN_DIR_2nd_mpnn)\n",
    "if len(glob.glob(DESIGN_DIR_ligMPNN+'/good/*.pdb')) == 0:\n",
    "    sys.exit(\"No designs to run 2nd MPNN on.\")\n",
    "\n",
    "## If you're done with design and happy with the outputs then mark it as done\n",
    "if not os.path.exists(DESIGN_DIR_2nd_mpnn+\"/.done\"):\n",
    "    with open(f\"{DESIGN_DIR_2nd_mpnn}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d495bf96",
   "metadata": {},
   "source": [
    "## 5.1 AlphaFold2 predictions on the 2nd MPNN run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(WDIR)\n",
    "assert os.path.exists(DESIGN_DIR_2nd_mpnn+\"/.done\"), \"2nd MPNN has not been performed!\"\n",
    "\n",
    "AF2_DIR = f\"{WDIR}/inpainting/5.1_2nd_af2\"\n",
    "os.makedirs(AF2_DIR, exist_ok=True)\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "### First collecting MPNN outputs and creating FASTA files for AF2 input\n",
    "mpnn_fasta = utils.parse_fasta_files(glob.glob(f\"{DESIGN_DIR_2nd_mpnn}/seqs/*.fa\"))\n",
    "# Giving sequences unique names based on input PDB name, temperature, and sequence identifier\n",
    "_mpnn_fasta = {}\n",
    "for k, seq in mpnn_fasta.items():\n",
    "    if \"model_path\" in k:\n",
    "        _mpnn_fasta[k.split(\",\")[0]+\"_native\"] = seq.strip()\n",
    "    else:\n",
    "        _mpnn_fasta[k.split(\",\")[0]+\"_\"+k.split(\",\")[2].replace(\" T=\", \"T\")+\"_0_\"+k.split(\",\")[1].replace(\" id=\", \"\")] = seq.strip()\n",
    "mpnn_fasta = {k:v for k,v in _mpnn_fasta.items()}\n",
    "\n",
    "print(f\"A total on {len(mpnn_fasta)} sequences will be predicted.\")\n",
    "\n",
    "## Splitting the MPNN sequences based on length\n",
    "## and grouping them in smaller batches for each AF2 job\n",
    "## Use group size of >40 when running on GPU. Also depends on how many sequences and resources you have.\n",
    "SEQUENCES_PER_AF2_JOB = 5  # CPU\n",
    "if USE_GPU_for_AF2 is True:\n",
    "    SEQUENCES_PER_AF2_JOB = 100  # GPU\n",
    "mpnn_fasta_split = utils.split_fasta_based_on_length(mpnn_fasta, SEQUENCES_PER_AF2_JOB, write_files=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up AlphaFold2 run\n",
    "\n",
    "AF2_recycles = 3\n",
    "AF2_models = \"4\"  # add other models to this string if needed\n",
    "\n",
    "commands_af2 = []\n",
    "cmds_filename_af2 = \"commands_af2\"\n",
    "with open(cmds_filename_af2, \"w\") as file:\n",
    "    for ff in glob.glob(\"*.fasta\"):\n",
    "        commands_af2.append(f\"{PYTHON['af2']} {AF2_script} \"\n",
    "                             f\"--af-nrecycles {AF2_recycles} --af-models {AF2_models} \"\n",
    "                             f\"--fasta {ff} --scorefile {ff.replace('.fasta', '.csv')}\\n\")\n",
    "        file.write(commands_af2[-1])\n",
    "\n",
    "print(\"Example AF2 command:\")\n",
    "print(commands_af2[-1])\n",
    "\n",
    "### Running AF2 with Slurm.\n",
    "### Running jobs on the CPU. It takes ~10 minutes per sequence\n",
    "\n",
    "submit_script = \"submit_af2.sh\"\n",
    "if USE_GPU_for_AF2 is True:\n",
    "    utils.create_slurm_submit_script(filename=submit_script, name=\"5.1_2nd_af2\", mem=\"6g\", \n",
    "                                     N_cores=2, gpu=True, gres=\"gpu:rtx2080:1\", time=\"00:12:00\", email=EMAIL, array=len(commands_af2),\n",
    "                                     array_commandfile=cmds_filename_af2)\n",
    "else:\n",
    "    utils.create_slurm_submit_script(filename=submit_script, name=\"5.1_2nd_af2\", mem=\"6g\", \n",
    "                                     N_cores=4, time=\"01:00:00\", email=EMAIL, array=len(commands_af2),\n",
    "                                     array_commandfile=cmds_filename_af2)\n",
    "\n",
    "if not os.path.exists(AF2_DIR+\"/.done\"):\n",
    "    p = subprocess.Popen(['sbatch', submit_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0915fdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you're done with AF2 and happy with the outputs then mark it as done\n",
    "AF2_DIR = f\"{WDIR}/inpainting/5.1_2nd_af2\"\n",
    "DESIGN_DIR_ligMPNN = f\"{WDIR}/inpainting/3.1_design_pocket_ligandMPNN\"\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "if not os.path.exists(AF2_DIR+\"/.done\"):\n",
    "    with open(f\"{AF2_DIR}/.done\", \"w\") as file:\n",
    "        file.write(f\"Run user: {username}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d53a010",
   "metadata": {},
   "source": [
    "### Analyzing AlphaFold2 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1266a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining all CSV scorefiles into one\n",
    "os.system(\"head -n 1 $(ls *aa*.csv | shuf -n 1) > scores.csv ; for f in *aa*.csv ; do tail -n +2 ${f} >> scores.csv ; done\")\n",
    "assert os.path.exists(\"scores.csv\"), \"Could not combine scorefiles\"\n",
    "\n",
    "### Calculating the RMSDs of AF2 predictions relative to the diffusion outputs\n",
    "### Catalytic residue sidechain RMSDs are calculated in the reference PDB has REMARK 666 line present\n",
    "\n",
    "analysis_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/utils/analyze_af2.py --scorefile scores.csv \"\\\n",
    "               f\"--ref_path {DESIGN_DIR_ligMPNN}/good/ --mpnn --params {' '.join(params)}\"\n",
    "\n",
    "p = subprocess.Popen(analysis_cmd, shell=True)\n",
    "(output, err) = p.communicate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981cc7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualizing and filtering AF2 results\n",
    "AF2_DIR = f\"{WDIR}/inpainting/5.1_2nd_af2\"\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "scores_af2 = pd.read_csv(\"scores.sc\", sep=\"\\s+\", header=0)\n",
    "\n",
    "### Filtering AF2 scores based on lddt and rmsd\n",
    "# Define your desired cutoffs here:\n",
    "AF2_filters = {\"lDDT\": [85.0, \">=\"],\n",
    "               \"rmsd\": [1.2, \"<=\"],\n",
    "               \"rmsd_SR1\": [1.0, \"<=\"]}  # 1st catalytic residue sc-rmsd\n",
    "\n",
    "scores_af2_filtered = utils.filter_scores(scores_af2, AF2_filters)\n",
    "utils.dump_scorefile(scores_af2_filtered, \"filtered_scores.sc\")\n",
    "\n",
    "## Plotting AF2 scores\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i,k in enumerate(AF2_filters):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.hist(scores_af2[k])\n",
    "    plt.title(k)\n",
    "    plt.xlabel(k)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "utils.plot_score_pairs(scores_af2, \"lDDT\", \"rmsd\", AF2_filters[\"lDDT\"][0], AF2_filters[\"rmsd\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Copying good predictions to a separate directory\n",
    "os.chdir(AF2_DIR)\n",
    "\n",
    "if len(scores_af2_filtered) > 0:\n",
    "    os.makedirs(\"good\", exist_ok=True)\n",
    "    good_af2_models = [row[\"Output_PDB\"]+\".pdb\" for idx,row in scores_af2_filtered.iterrows()]\n",
    "    for pdb in good_af2_models:\n",
    "        copy2(pdb, f\"good/{pdb}\")\n",
    "    good_af2_models = glob.glob(f\"{AF2_DIR}/good/*.pdb\")\n",
    "else:\n",
    "    sys.exit(\"No good models to continue this pipeline with\")\n",
    "\n",
    "os.chdir(f\"{AF2_DIR}/good\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d059ff96",
   "metadata": {},
   "source": [
    "## 6.1: Final FastRelax with the ligand\n",
    "Relaxing good AF2 models together with the ligand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7accf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "AF2_DIR = f\"{WDIR}/inpainting/5.1_2nd_af2\"\n",
    "DESIGN_DIR_ligMPNN = f\"{WDIR}/inpainting/3.1_design_pocket_ligandMPNN\"\n",
    "assert len(glob.glob(f\"{AF2_DIR}/good/*.pdb\")) > 0, \"No good AF2 models to relax with\"\n",
    "\n",
    "os.chdir(WDIR)\n",
    "RELAX_DIR = f\"{WDIR}/inpainting/6.1_final_relax\"\n",
    "os.makedirs(RELAX_DIR, exist_ok=True)\n",
    "os.chdir(RELAX_DIR)\n",
    "\n",
    "\n",
    "## First matching up the AF2 output filenames of step 5 with pocket design filenames from step 3\n",
    "ref_and_model_pairs = []\n",
    "for r in glob.glob(f\"{DESIGN_DIR_ligMPNN}/good/*.pdb\"):\n",
    "    for pdbfile in glob.glob(f\"{AF2_DIR}/good/*.pdb\"):\n",
    "        if os.path.basename(r).replace(\".pdb\", \"_\") in pdbfile:\n",
    "            ref_and_model_pairs.append((r, pdbfile))\n",
    "\n",
    "assert len(ref_and_model_pairs) == len(glob.glob(f\"{AF2_DIR}/good/*.pdb\")), \"Was not able to match all models with reference structures\"\n",
    "\n",
    "\n",
    "## Generating commands for relax jobs\n",
    "### Performing 1 relax iteration on each input structure\n",
    "NSTRUCT = 1\n",
    "cstfile = f\"{SCRIPT_DIR}/theozyme/HBA/HBA_CYS_UPO.cst\"\n",
    "\n",
    "commands_relax = []\n",
    "cmds_filename_rlx = \"commands_design\"\n",
    "with open(cmds_filename_rlx, \"w\") as file:\n",
    "    for r_m in ref_and_model_pairs:\n",
    "        commands_relax.append(f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/design/align_add_ligand_relax.py \"\n",
    "                              f\"--outdir ./ --ligand {LIGAND} --ref_pdb {r_m[0]}\"\n",
    "                              f\"--pdb {r_m[1]} --nstruct {NSTRUCT} \"\n",
    "                              f\"--params {' '.join(params)} --cstfile {cstfile} > logs/{os.path.basename(pdb).replace('.pdb', '.log')}\\n\")\n",
    "        file.write(commands_relax[-1])\n",
    "\n",
    "print(\"Example design command:\")\n",
    "print(commands_design[-1])\n",
    "\n",
    "\n",
    "### Running design jobs with Slurm.\n",
    "submit_script = \"submit_relax.sh\"\n",
    "utils.create_slurm_submit_script(filename=submit_script, name=\"6.1_final_relax\", mem=\"4g\", \n",
    "                                 N_cores=1, time=\"0:30:00\", email=EMAIL, array=len(commands_relax),\n",
    "                                 array_commandfile=cmds_filename_rlx)\n",
    "\n",
    "if not os.path.exists(RELAX_DIR+\"/.done\"):\n",
    "    p = subprocess.Popen(['sbatch', submit_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    (output, err) = p.communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f57db8",
   "metadata": {},
   "source": [
    "### Analyzing final relaxed structures\n",
    "Filtering them based on the same metrics as was used for the initial design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b11d1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyzing Rosetta designs\n",
    "RELAX_DIR = f\"{WDIR}/inpainting/6.1_final_relax\"\n",
    "os.chdir(RELAX_DIR)\n",
    "\n",
    "scores = pd.read_csv(\"scorefile.txt\", sep=\"\\s+\", header=0)\n",
    "\n",
    "filters = {'all_cst': [1.0, '<='],\n",
    " 'nlr_SR1_rms': [0.8, '<='],\n",
    " 'nlr_totrms': [1.0, '<='],\n",
    " 'L_SASA': [0.2, '<='],\n",
    " 'COO_hbond': [1.0, '='],\n",
    " 'heme_angle_wrst': [80.0, '>='],\n",
    " 'score_per_res': [0.0, '<='],\n",
    " 'corrected_ddg': [-50.0, '<='],\n",
    " 'cms_per_atom': [4.8, '>='],\n",
    " 'rmsd_CA_rlx_in': [1.0, \"<=\"]}  # rmsd_CA_rlx_in is rmsd between relaxed structure and AF2 prediction\n",
    "\n",
    "filtered_scores = utils.filter_scores(scores, filters)\n",
    "\n",
    "## Plotting relax scores\n",
    "plt.figure(figsize=(12, 9))\n",
    "for i,k in enumerate(filters):\n",
    "    if k not in scores.keys():\n",
    "        continue\n",
    "    plt.subplot(4, 3, i+1)\n",
    "    plt.hist(scores[k])\n",
    "    plt.title(k)\n",
    "    plt.xlabel(k)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "### Copying good designs over to a new directory\n",
    "if len(filtered_scores) > 0:\n",
    "    os.makedirs(f\"{RELAX_DIR}/good\", exist_ok=True)\n",
    "    for idx, row in filtered_scores.iterrows():\n",
    "        copy2(row[\"description\"]+\".pdb\", \"good/\"+row[\"description\"]+\".pdb\")\n",
    "else:\n",
    "    print(\"No good designs created, bummer...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba174b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(filtered_scores) > 0:\n",
    "    print(f\"CONGRATULATIONS! You have successfully designed {len(filtered_scores)} proteins against ligand {LIGAND}\")\n",
    "    print(\"You can find the design models in the directory:\\n\"\n",
    "          f\"    {RELAX_DIR}/good\")\n",
    "    print(\"\\nIt is advised you manually inspect them before ordering.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
